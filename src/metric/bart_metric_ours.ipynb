{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/aparna/miniconda3/envs/new/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# bertscore = load(\"bertscore\") \n",
    "tokenizer =  AutoTokenizer.from_pretrained('facebook/bart-base',cahe_dir='/ssd_scratch/cvit/aparna/bart')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/ssd_scratch/cvit/aparna/mbart_simplification_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "\n",
    "def tokenize_sentence(arg):\n",
    "    encoded_arg =tokenizer(arg)\n",
    "    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    print(len(eval_arg[0]),len(eval_arg[1]),len(eval_arg[2]))\n",
    "    print(len(eval_arg))\n",
    "    text_inputs = eval_arg[0]\n",
    "    text_preds = eval_arg[1]\n",
    "    text_labels = eval_arg[2]\n",
    "    texts_bleu =[text.strip() for text in text_preds]\n",
    "    labels_bleu = [[text.strip()] for text in text_labels[0]]\n",
    "    result = bleu_metric.compute(predictions=texts_bleu, references=text_labels)\n",
    "    return result[\"score\"],sari_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        sources=text_inputs,\n",
    "    )['sari']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/home2/aparna/miniconda3/envs/new/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home2/aparna/miniconda3/envs/new/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "  0%|          | 1/10000 [00:01<3:51:10,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n",
      "3\n",
      "(25.926559358625884, 40.81037555988784)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/10000 [01:47<4:43:40,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1010 1010 1010\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 101/10000 [01:49<4:52:12,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36.753112969676955, 46.65782938074665)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 200/10000 [03:57<2:52:26,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010 2010 2010\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 201/10000 [04:00<4:22:29,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35.97689666111966, 45.551752576331126)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 300/10000 [07:54<6:33:27,  2.43s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3010 3010 3010\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 301/10000 [08:01<10:26:48,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35.94534599449593, 45.7934274797807)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 400/10000 [12:19<10:34:14,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4010 4010 4010\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 401/10000 [12:29<15:41:00,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36.15792048408291, 44.85257431335065)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 435/10000 [14:05<5:40:07,  2.13s/it] "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_csv(\"../../data/10/test.csv\")\n",
    "test_data = test_data.dropna()\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "#take 1000 samples\n",
    "test_data = test_data[:10000]\n",
    "texts = test_data[\"source\"].tolist()\n",
    "labels = test_data[\"target\"].tolist()\n",
    "\n",
    "\n",
    "metrics =[]\n",
    "inpu = []\n",
    "cands = []\n",
    "lab = []\n",
    "max_l = 512\n",
    "num_b = 10\n",
    "num_sub_b =1\n",
    "\n",
    "for  i  in tqdm(range(len(texts))):\n",
    "    text = texts[i]\n",
    "    # encode the text into tensor of integers using the appropriate tokenizer\n",
    "    inputs = tokenizer.encode(\"paraphrase: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    beam_outputs = model.generate(inputs,max_length=max_l,num_beams=num_b,early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=num_b,\n",
    "        top_k=4, top_p=0.95\n",
    "        # return_dict_in_generate=True,\n",
    "    )\n",
    "   # print(\"====================================\")\n",
    "    inpu.extend([text]*10)\n",
    "    c =[]\n",
    "    for x, beam in enumerate(beam_outputs):\n",
    "        #print(\"{}\".format(i, tokenizer.decode(beam, skip_special_tokens=True)))\n",
    "        c.append(tokenizer.decode(beam, skip_special_tokens=True))\n",
    "    cands.extend(c)\n",
    "    lab.extend([[labels[i]]]*10)\n",
    "    eval_arg = [inpu,cands,lab]\n",
    "    if i%100==0:\n",
    "        metric = metrics_func(eval_arg)\n",
    "        print(metric)\n",
    "\n",
    "\n",
    "print(len(inpu),len(cands),len(lab))\n",
    "metric = metrics_func(eval_arg)\n",
    "print(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_csv(\"../../data/10/test.csv\")\n",
    "test_data = test_data.dropna()\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "#take 1000 samples\n",
    "test_data = test_data[:10000]\n",
    "texts = test_data[\"source\"].tolist()\n",
    "labels = test_data[\"target\"].tolist()\n",
    "\n",
    "\n",
    "metrics =[]\n",
    "inpu = []\n",
    "cands = []\n",
    "lab = []\n",
    "max_l = 512\n",
    "num_b = 10\n",
    "num_sub_b =1\n",
    "\n",
    "for  i  in tqdm(range(len(texts))):\n",
    "    text = texts[i]\n",
    "    # encode the text into tensor of integers using the appropriate tokenizer\n",
    "    inputs = tokenizer.encode(\"paraphrase: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    beam_outputs = model.generate(inputs,max_length=max_l,num_beams=num_b,early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=1,\n",
    "        top_k=4, top_p=0.95\n",
    "        # return_dict_in_generate=True,\n",
    "    )\n",
    "   # print(\"====================================\")\n",
    "    inpu.extend([text]*1)\n",
    "    c =[]\n",
    "    for x, beam in enumerate(beam_outputs):\n",
    "        #print(\"{}\".format(i, tokenizer.decode(beam, skip_special_tokens=True)))\n",
    "        c.append(tokenizer.decode(beam, skip_special_tokens=True))\n",
    "    cands.extend(c)\n",
    "    lab.extend([[labels[i]]]*1)\n",
    "    eval_arg = [inpu,cands,lab]\n",
    "    if i%100==0:\n",
    "        metric = metrics_func(eval_arg)\n",
    "        print(metric)\n",
    "\n",
    "\n",
    "print(len(inpu),len(cands),len(lab))\n",
    "metric = metrics_func(eval_arg)\n",
    "print(metric)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
