{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/aparna/miniconda3/envs/new/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import csv\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "#Encoder-Decoder Model\n",
    "from transformers import EncoderDecoderModel\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Trainer\n",
    "from evaluate import load\n",
    "#Training\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from transformers import  Seq2SeqTrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "# bertscore = load(\"bertscore\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-small-finetuned-text-simplification\",cache_dir =\"/scratch/aparna\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-small-finetuned-text-simplification\",cache_dir =\"/scratch/aparna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "\n",
    "def tokenize_sentence(arg):\n",
    "    encoded_arg =tokenizer(arg)\n",
    "    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    print(len(eval_arg[0]),len(eval_arg[1]),len(eval_arg[2]))\n",
    "    print(len(eval_arg))\n",
    "    text_inputs = eval_arg[0]\n",
    "    text_preds = eval_arg[1]\n",
    "    text_labels = eval_arg[2]\n",
    "    texts_bleu =[text.strip() for text in text_preds]\n",
    "    labels_bleu = [[text.strip()] for text in text_labels[0]]\n",
    "    result = bleu_metric.compute(predictions=texts_bleu, references=text_labels)\n",
    "    return result[\"score\"],sari_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        sources=text_inputs,\n",
    "    )['sari']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [2:19:56<00:00,  1.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 100000 100000\n",
      "100000 100000 100000\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.38025112022054053, 35.48340179756404)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_csv(\"../../data/10/test.csv\")\n",
    "test_data = test_data.dropna()\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "#take 1000 samples\n",
    "test_data = test_data[:10000]\n",
    "texts = test_data[\"source\"].tolist()\n",
    "labels = test_data[\"target\"].tolist()\n",
    "\n",
    "\n",
    "metrics =[]\n",
    "inpu = []\n",
    "cands = []\n",
    "lab = []\n",
    "max_l = 512\n",
    "num_b = 10\n",
    "num_sub_b =1\n",
    "\n",
    "for  i  in tqdm(range(len(texts))):\n",
    "    text = texts[i]\n",
    "    # encode the text into tensor of integers using the appropriate tokenizer\n",
    "    inputs = tokenizer.encode(\"paraphrase: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    beam_outputs = model.generate(inputs,max_length=max_l,num_beams=num_b,early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=num_b,\n",
    "        top_k=4, top_p=0.95\n",
    "        # return_dict_in_generate=True,\n",
    "    )\n",
    "   # print(\"====================================\")\n",
    "    inpu.extend([text]*10)\n",
    "    c =[]\n",
    "    for x, beam in enumerate(beam_outputs):\n",
    "        #print(\"{}\".format(i, tokenizer.decode(beam, skip_special_tokens=True)))\n",
    "        c.append(tokenizer.decode(beam, skip_special_tokens=True))\n",
    "    cands.extend(c)\n",
    "    lab.extend([[labels[i]]]*10)\n",
    "    eval_arg = [inpu,cands,lab]\n",
    "\n",
    "\n",
    "print(len(inpu),len(cands),len(lab))\n",
    "metric = metrics_func(eval_arg)\n",
    "print(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/home2/aparna/miniconda3/envs/new/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home2/aparna/miniconda3/envs/new/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      " 19%|█▉        | 1940/10000 [31:23<4:24:52,  1.97s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_csv(\"../../data/10/test.csv\")\n",
    "test_data = test_data.dropna()\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "#take 1000 samples\n",
    "test_data = test_data[:10000]\n",
    "texts = test_data[\"source\"].tolist()\n",
    "labels = test_data[\"target\"].tolist()\n",
    "\n",
    "\n",
    "metrics =[]\n",
    "inpu = []\n",
    "cands = []\n",
    "lab = []\n",
    "max_l = 512\n",
    "num_b = 10\n",
    "num_sub_b =1\n",
    "\n",
    "for  i  in tqdm(range(len(texts))):\n",
    "    text = texts[i]\n",
    "    # encode the text into tensor of integers using the appropriate tokenizer\n",
    "    inputs = tokenizer.encode(\"paraphrase: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    beam_outputs = model.generate(inputs,max_length=max_l,num_beams=num_b,early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=1,\n",
    "        top_k=4, top_p=0.95\n",
    "        # return_dict_in_generate=True,\n",
    "    )\n",
    "   # print(\"====================================\")\n",
    "    inpu.extend([text]*1)\n",
    "    c =[]\n",
    "    for x, beam in enumerate(beam_outputs):\n",
    "        #print(\"{}\".format(i, tokenizer.decode(beam, skip_special_tokens=True)))\n",
    "        c.append(tokenizer.decode(beam, skip_special_tokens=True))\n",
    "    cands.extend(c)\n",
    "    lab.extend([[labels[i]]]*1)\n",
    "    eval_arg = [inpu,cands,lab]\n",
    "\n",
    "\n",
    "print(len(inpu),len(cands),len(lab))\n",
    "metric = metrics_func(eval_arg)\n",
    "print(metric)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svoice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
