{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "from transformers import BartForConditionalGeneration, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "\n",
    "\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    print(len(eval_arg[0]),len(eval_arg[1]),len(eval_arg[2]))\n",
    "    print(len(eval_arg))\n",
    "    text_inputs = eval_arg[0]\n",
    "    text_preds = eval_arg[1]\n",
    "    text_labels = eval_arg[2]\n",
    "    texts_bleu =[text.strip() for text in text_preds]\n",
    "    labels_bleu = [[text.strip()] for text in text_labels[0]]\n",
    "    result = bleu_metric.compute(predictions=texts_bleu, references=text_labels)\n",
    "    return result[\"score\"],sari_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        sources=text_inputs,\n",
    "    )['sari']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]/home2/aparna/miniconda3/envs/new/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home2/aparna/miniconda3/envs/new/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "  0%|          | 1/10000 [00:32<91:29:50, 32.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n",
      "3\n",
      "(34.61004551647818, 39.15192624769725)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 73/10000 [31:24<196:58:36, 71.43s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "classifier = RobertaForSequenceClassification.from_pretrained(\"liamcripwell/ctrl44-clf\",cache_dir='/ssd_scratch/cvit/aparna/classification')\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"liamcripwell/ctrl44-clf\",cache_dir='/ssd_scratch/cvit/aparna/classification_tokenizer')\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"liamcripwell/ctrl44-simp\",cache_dir='/ssd_scratch/cvit/aparna/paraphrase')\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"liamcripwell/ctrl44-simp\",cache_dir='/ssd_scratch/cvit/aparna/paraphrase_tokenizer')\n",
    "\n",
    "\n",
    "test_data = pd.read_csv(\"../../data/10/test.csv\")\n",
    "test_data = test_data.dropna()\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "#take 1000 samples\n",
    "test_data = test_data[:10000]\n",
    "texts = test_data[\"source\"].tolist()\n",
    "labels = test_data[\"target\"].tolist()\n",
    "\n",
    "metrics =[]\n",
    "inpu = []\n",
    "cands = []\n",
    "lab = []\n",
    "max_l = 512\n",
    "num_b = 10\n",
    "num_sub_b =1\n",
    "\n",
    "for  i  in tqdm(range(len(texts))):\n",
    "    text = texts[i]\n",
    "    inputs = tokenizer1(text, return_tensors=\"pt\")\n",
    "    inpu.extend([text]*10)\n",
    "    with torch.no_grad():\n",
    "        logits = classifier(**inputs).logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "        predicted_class_name = classifier.config.id2label[predicted_class_id]\n",
    "        text = predicted_class_name + \" \" + text\n",
    "        inputs1 = tokenizer2(text, return_tensors=\"pt\")\n",
    "        beam_outputs = model.generate(**inputs1,max_length=max_l,num_beams=num_b,early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=10,\n",
    "        top_k=4, top_p=0.95\n",
    "        # return_dict_in_generate=True,\n",
    "    )\n",
    "    c = []\n",
    "    for x, beam in enumerate(beam_outputs):\n",
    "    #print(\"{}\".format(i, tokenizer.decode(beam, skip_special_tokens=True)))\n",
    "        c.append(tokenizer2.decode(beam, skip_special_tokens=True))\n",
    "    cands.extend(c)\n",
    "\n",
    "    lab.extend([[labels[i]]]*10)\n",
    "    eval_arg = [inpu,cands,lab]\n",
    "    #print(i)\n",
    "    if i%100==0:\n",
    "        metric = metrics_func(eval_arg)\n",
    "        metrics.append(metric)\n",
    "        print(metric)\n",
    "        \n",
    "print(len(inpu),len(cands),len(lab))\n",
    "metric = metrics_func(eval_arg)\n",
    "print(metric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"liamcripwell/ctrl44-clf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"liamcripwell/ctrl44-clf\")\n",
    "\n",
    "text = \"Barack Hussein Obama II is an American politician who served as the 44th president of the United States from 2009 to 2017.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "  logits = model(**inputs).logits\n",
    "predicted_class_id = logits.argmax().item()\n",
    "predicted_class_name = model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5020 5010 5010\n",
      "5020 5010 5010\n",
      "3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatch in the number of sources (5020) and predictions (5010)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home2/aparna/ANLP-Poject/src/metric/classify_and_paraprase.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode040/home2/aparna/ANLP-Poject/src/metric/classify_and_paraprase.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(inpu),\u001b[39mlen\u001b[39m(cands),\u001b[39mlen\u001b[39m(lab))\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgnode040/home2/aparna/ANLP-Poject/src/metric/classify_and_paraprase.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m metric \u001b[39m=\u001b[39m metrics_func(eval_arg)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgnode040/home2/aparna/ANLP-Poject/src/metric/classify_and_paraprase.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(metric)\n",
      "\u001b[1;32m/home2/aparna/ANLP-Poject/src/metric/classify_and_paraprase.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode040/home2/aparna/ANLP-Poject/src/metric/classify_and_paraprase.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m labels_bleu \u001b[39m=\u001b[39m [[text\u001b[39m.\u001b[39mstrip()] \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m text_labels[\u001b[39m0\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode040/home2/aparna/ANLP-Poject/src/metric/classify_and_paraprase.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m result \u001b[39m=\u001b[39m bleu_metric\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mtexts_bleu, references\u001b[39m=\u001b[39mtext_labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgnode040/home2/aparna/ANLP-Poject/src/metric/classify_and_paraprase.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result[\u001b[39m\"\u001b[39m\u001b[39mscore\u001b[39m\u001b[39m\"\u001b[39m],sari_metric\u001b[39m.\u001b[39;49mcompute(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode040/home2/aparna/ANLP-Poject/src/metric/classify_and_paraprase.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     predictions\u001b[39m=\u001b[39;49mtext_preds,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode040/home2/aparna/ANLP-Poject/src/metric/classify_and_paraprase.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     references\u001b[39m=\u001b[39;49mtext_labels,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode040/home2/aparna/ANLP-Poject/src/metric/classify_and_paraprase.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     sources\u001b[39m=\u001b[39;49mtext_inputs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode040/home2/aparna/ANLP-Poject/src/metric/classify_and_paraprase.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m )[\u001b[39m'\u001b[39m\u001b[39msari\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/new/lib/python3.8/site-packages/evaluate/module.py:450\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m compute_kwargs \u001b[39m=\u001b[39m {k: kwargs[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m kwargs \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feature_names()}\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m--> 450\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_batch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_finalize()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_file_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/new/lib/python3.8/site-packages/evaluate/module.py:541\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m     error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    536\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPredictions and/or references don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt match the expected format.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected format: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselected_feature_format\u001b[39m \u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput predictions: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[39m}\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput references: \u001b[39m\u001b[39m{\u001b[39;00msummarize_if_long_list(references)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    540\u001b[0m     )\n\u001b[0;32m--> 541\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch in the number of sources (5020) and predictions (5010)"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(inpu),len(cands),len(lab))\n",
    "metric = metrics_func(eval_arg)\n",
    "print(metric)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
