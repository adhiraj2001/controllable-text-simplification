{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33801/33801 [00:18<00:00, 1863.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83258 83258 83258\n",
      "83258 83258 83258\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8.680953594011816, 32.834411652484036)\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "sari_metric = evaluate.load(\"sari\")\n",
    "\n",
    "def tokenize_sentence(arg):\n",
    "    encoded_arg =tokenizer(arg)\n",
    "    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    print(len(eval_arg[0]),len(eval_arg[1]),len(eval_arg[2]))\n",
    "    print(len(eval_arg))\n",
    "    text_inputs = eval_arg[0]\n",
    "    text_preds = eval_arg[1]\n",
    "    text_labels = eval_arg[2]\n",
    "    texts_bleu =[text.strip() for text in text_preds]\n",
    "    labels_bleu = [[text.strip()] for text in text_labels[0]]\n",
    "    result = bleu_metric.compute(predictions=texts_bleu, references=text_labels)\n",
    "    return result[\"score\"],sari_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        sources=text_inputs,\n",
    "    )['sari']\n",
    "\n",
    "# %%\n",
    "from torch.utils.data import DataLoader\n",
    "#tokenizer = MT5Tokenizer.from_pretrained(\"./mt5\")\n",
    "\n",
    "#load json\n",
    "with open('../../data/10/dissim_dict.json') as f:\n",
    "    dissim_dict = json.load(f)\n",
    "\n",
    "with open('../../data/10/input.txt') as f:\n",
    "    inputs = f.readlines()\n",
    "\n",
    "with open('../../data/10/labels.txt') as f:\n",
    "    labels = f.readlines()\n",
    "\n",
    "metrics =[]\n",
    "inpu = []\n",
    "cands = []\n",
    "lab = []\n",
    "for i in tqdm(inputs):\n",
    "        inpu.extend([i.strip()]*len(dissim_dict[i.strip()]))\n",
    "        cands.extend(dissim_dict[i.strip()])\n",
    "        lab.extend([[labels[inputs.index(i)]]]*len(dissim_dict[i.strip()]))\n",
    "        #print(len(inpu),len(cands),len(lab))\n",
    "        eval_arg = [inpu,cands,lab]\n",
    "        #metric = metrics_func(eval_arg)\n",
    "        #print(metric)\n",
    "        #print(\"++++++++++++++++++++++++++++++\")\n",
    "        #metrics.append(metric)\n",
    "\n",
    "print(len(inpu),len(cands),len(lab))\n",
    "metric = metrics_func(eval_arg)\n",
    "print(metric)\n",
    "\n",
    "# def average_metric(metrics):\n",
    "#     rouge = 0\n",
    "#     rouge2 = 0\n",
    "#     rougeL = 0\n",
    "#     rougeLsum = 0\n",
    "#     bleu = 0\n",
    "#     sari =0\n",
    "#     for metric in metrics:\n",
    "#         # rouge += metric[0]['rouge1']\n",
    "#         # rouge2 += metric[0]['rouge2']\n",
    "#         # rougeL += metric[0]['rougeL']\n",
    "#         # rougeLsum += metric[0]['rougeLsum']\n",
    "#         bleu += metric[0]\n",
    "#         sari += metric[1]\n",
    "\n",
    "#     return bleu/len(metrics),sari/len(metrics)\n",
    "      \n",
    "\n",
    "# print(\"t5_small\")\n",
    "# scores = average_metric(metrics)\n",
    "# print(\"rouge:\",scores[0])\n",
    "# print(\"rouge2:\",scores[1])\n",
    "# print(\"rougeL:\",scores[2])\n",
    "# print(\"rougeLsum:\",scores[3])\n",
    "# print(\"bleu:\",scores[4])\n",
    "\n",
    "\n",
    "# # %%\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# Predict with test data (first 5 rows)\n",
    "# sample_dataloader = DataLoader(\n",
    "#   test.with_format(\"torch\"),\n",
    "#   collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "#   batch_size=5)\n",
    "# for batch in sample_dataloader:\n",
    "#   with torch.no_grad():\n",
    "#     preds = model.generate(\n",
    "#       batch[\"input_ids\"],\n",
    "#       num_beams=15,\n",
    "#       num_return_sequences=1,\n",
    "#       no_repeat_ngram_size=1,\n",
    "#       remove_invalid_values=True,\n",
    "#       max_length=128,\n",
    "#     )\n",
    "#   labels = batch[\"labels\"]\n",
    "#   inputs = batch[\"input_ids\"]\n",
    "#   break\n",
    "\n",
    "# # Replace -100 (see above)\n",
    "# inputs = np.where(inputs != -100, inputs, tokenizer.pad_token_id)\n",
    "# labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "# # Convert id tokens to text\n",
    "# text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "# text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "# text_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "# #print(bleu_score(list(text_labels.split()),list(text_preds.split())))\n",
    "\n",
    "# # Show result\n",
    "# print(\"***** Input's Text *****\")\n",
    "# print(text_inputs[2])\n",
    "# print(\"***** summmary (True Value) *****\")\n",
    "# print(text_labels[2])\n",
    "# print(\"***** summary (Generated Text) *****\")\n",
    "# print(text_preds[2])\n",
    "\n",
    "# # %%\n",
    "# for i in range(5):\n",
    "#     print(\"***** Input's Text *****\")\n",
    "#     print(text_inputs[i])\n",
    "#     print(\"***** summarry (True Value) *****\")\n",
    "#     print(text_labels[i])\n",
    "#     print(\"***** summary (Generated Text) *****\")\n",
    "#     print(text_preds[i])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svoice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
