{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e408d70f-7c51-4e72-86a4-ceefa7135d96",
   "metadata": {},
   "source": [
    "# Seq2Seq Text Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7f5bfe-4471-42ea-a650-c06b62d48a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import yaml\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "# wandb.login(key='913841cb22c908099db4951c258f4242c1d1b7aa')\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_API_KEY'] = '913841cb22c908099db4951c258f4242c1d1b7aa'\n",
    "os.environ['WANDB_SILENT'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1703215-e73e-4bd7-8b09-a66632dd7953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## To avoid Cuda out of Memory Error (if doesn't work, try reducing batch size)\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564900f-cffa-4172-85d2-b456792e0715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune mt5 on dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import  AutoTokenizer, BertGenerationDecoder, BertGenerationEncoder,BertGenerationConfig,EncoderDecoderModel\n",
    "from transformers import XLMTokenizer, XLMForSequenceClassification\n",
    "\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer, BertTokenizer, BertModel\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "\n",
    "import klib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c697c86-6716-4af1-b213-56aa2ed48d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-05T12:44:59.927434Z",
     "iopub.status.busy": "2023-11-05T12:44:59.926885Z",
     "iopub.status.idle": "2023-11-05T12:44:59.934678Z",
     "shell.execute_reply": "2023-11-05T12:44:59.932937Z",
     "shell.execute_reply.started": "2023-11-05T12:44:59.927383Z"
    }
   },
   "source": [
    "### 1. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad99bd-273d-42be-a5bf-b11526402bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "\n",
    "# # Load 1% of the training/validation sets.\n",
    "# train_data      = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[0:1%]\")\n",
    "# validation_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[0:1%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c884f7-cf23-43c5-8132-02b1ed28d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_root = '/ssd_scratch/cvit/adhiraj_deshmukh'\n",
    "abs_code = f'{abs_root}/ANLP-Project'\n",
    "abs_data = f'{abs_code}/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e92de3-15c9-4d20-9ceb-9ff2705208d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "colnames = ['source', 'target']\n",
    "\n",
    "# input_file = \"train.tsv\"\n",
    "input_file = f\"{abs_data}/train.tsv\"\n",
    "train = pd.read_csv(input_file, sep=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8', header=None, names=colnames)\n",
    "val = pd.read_csv(f\"{abs_data}/valid.tsv\", sep=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8', header=None, names=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc918543-bdc5-491c-a3f7-f07128be1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning \n",
    "\n",
    "train=klib.data_cleaning(train)\n",
    "val=klib.data_cleaning(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fdf1a6-06bd-4e4f-8610-f377bcef0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train, val, test\n",
    "# convert df  so that it can be used by transformers\n",
    "\n",
    "train, test = train_test_split(train, test_size=0.2, random_state=42)\n",
    "#train, val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873cd21f-c66c-4971-a5d8-0d8900871f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print lens\n",
    "print(len(train))\n",
    "print(len(val))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5a5e2-281b-4023-a5be-8f85fad5bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save train, val, test\n",
    "train.to_csv(f'{abs_data}/train.csv', index=False)\n",
    "val.to_csv(f'{abs_data}/val.csv', index=False)\n",
    "test.to_csv(f'{abs_data}/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa83ea-b794-480a-b00f-e3b33132c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66590a46-e123-4227-be09-67c4f998203e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22fb1c58-9abd-4de7-aeed-553b92872ff7",
   "metadata": {},
   "source": [
    "### 2. Tokenize and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c4f54-b5d8-4cba-a25d-781c0a358dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "prefix = \"Summarize: \"\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "batch_size = 16 # [4, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc0e060-c6f6-4a82-84ec-1d582f4d5d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the BART's pre-trained Tokenizer\n",
    "\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', cache_dir=f'{abs_root}/bart_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58e987-c19f-4863-9f7a-04918678da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    sentences = nltk.sent_tokenize(text.strip())\n",
    "    sentences_cleaned = [s for sent in sentences for s in sent.split(\"\\n\")]\n",
    "    sentences_cleaned_no_titles = [sent for sent in sentences_cleaned\n",
    "                                 if len(sent) > 0 and\n",
    "                                 sent[-1] in string.punctuation]\n",
    "    \n",
    "    text_cleaned = \"\\n\".join(sentences_cleaned_no_titles)\n",
    "    return text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd4613-45e6-4794-9c78-91612ca502c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to make the correct data structure\n",
    "def process_data_to_model_inputs(batch):\n",
    "    inputs = [prefix + f\"\\\"clean_text(text)\\\"\" for text in batch[\"source\"]]\n",
    "    model_inputs = tokenizer(inputs, padding=\"max_length\", max_length=max_input_length, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        outputs = [clean_text(text) for text in batch[\"target\"]]\n",
    "        model_outputs = tokenizer(outputs, padding=\"max_length\", max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    batch[\"input_ids\"] = model_inputs.input_ids\n",
    "    batch[\"attention_mask\"] = model_inputs.attention_mask\n",
    "    \n",
    "    batch[\"decoder_input_ids\"] = model_outputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = model_outputs.attention_mask\n",
    "    \n",
    "    batch[\"labels\"] = model_outputs.input_ids.copy()\n",
    "    \n",
    "    # We have to make sure that the PAD token is ignored for calculating the loss\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1e2a8-e50a-47ca-8207-cd9d340b401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_dataset('csv', data_files=f'{abs_data}/train.csv',cache_dir=f'{abs_root}/t5_data')\n",
    "val = load_dataset('csv', data_files=f'{abs_data}/val.csv',cache_dir=f'{abs_root}/t5_data')\n",
    "test = load_dataset('csv', data_files=f'{abs_data}/test.csv',cache_dir=f'{abs_root}/t5_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7ede6-8eba-4c62-b3a8-9992e9a5d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"validation\"] = val[\"train\"]\n",
    "train[\"test\"] = test[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddebbf-986d-4e6c-b003-4504529326f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"train\"] = train[\"train\"].shuffle().select(range(100000))\n",
    "train[\"validation\"] = train[\"validation\"].shuffle().select(range(1000))\n",
    "train[\"test\"] = train[\"test\"].shuffle().select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045965b-0479-4e84-b1b5-ebaeeeaf70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the function to both train/validation sets.\n",
    "train = train.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True,\n",
    "    # remove_columns=[\"source\", \"target\"], batch_size = batch_size\n",
    "    remove_columns=[\"source\", \"target\"], batch_size = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7de8f4-e03e-4329-9298-281ab35387ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Dataset to PyTorch tensor with the expected columns\n",
    "train.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\",\n",
    "                           \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d671f-b518-42cb-bb06-76cf26b82f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the iterative object that does batching using the DataLoader\n",
    "train_dl = DataLoader(train[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(train[\"validation\"], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c97235-d653-4bd7-b317-b0091c2314d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47064b11-5bba-4036-8863-f6e237711999",
   "metadata": {},
   "source": [
    "### 3. Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0f2c1-c690-49a4-aaff-c4a16ff66bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\", cache_dir=f'{abs_root}/bart_base')\n",
    "model = model.to(device)\n",
    "\n",
    "# Split model's components\n",
    "the_encoder = model.get_encoder()\n",
    "the_decoder = model.get_decoder()\n",
    "\n",
    "last_linear_layer = model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7c450-4fb8-4677-b1b9-0aaf325508ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure why this is done ?\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee8309-5b4e-4748-ba2b-f640cc5ef5f0",
   "metadata": {},
   "source": [
    "### 4. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e289c92-b208-4e94-bf78-7b479caca000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3 # [3, 10]\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "\n",
    "learning_rate = 5e-5 # [5e-5, 5e-4]\n",
    "lr_scheduler_type = \"linear\"\n",
    "\n",
    "warmup_steps = 0\n",
    "# optim = \"adafactor\"\n",
    "# weight_decay = 0.01\n",
    "\n",
    "## The loss function\n",
    "loss_fct =  nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "## The optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "# optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
    "\n",
    "lr_scheduler = get_scheduler (\n",
    "    lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "# lr_scheduler = AdafactorSchedule(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9897fb5-911d-4e0f-ba9e-1e1e5a90f191",
   "metadata": {},
   "source": [
    "### 5. Freeze Layer for Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077b1e8-16e8-47ee-b87b-f0c2a7ec21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the first n-2 layers\n",
    "for i in range(len(model.model.encoder.layers) - 2):\n",
    "    for param in model.model.encoder.layers[i].parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "for i in range(len(model.model.decoder.layers) - 2):\n",
    "    for param in model.model.decoder.layers[i].parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb629a1-56c4-455d-8fc4-dcfbd436a1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e516341e-d5eb-4468-b43a-80170834a1be",
   "metadata": {},
   "source": [
    "### 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a540c7-ec55-432b-9958-02e806d3bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"ANLP-Project\",\n",
    "    # name=\"BART Finetune on Wiki-Auto\",\n",
    "    config={\n",
    "        \"architecture\": \"BART-Checkpoint\",\n",
    "        \"dataset\": \"Wiki-Auto\",\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        # # \"gamma\": GAMMA,\n",
    "        # # \"step_size\": STEP_SIZE\n",
    "        # \"factor\": FACTOR,\n",
    "        # \"patience\": PATIENCE,\n",
    "        # \"log_step\": LOG_STEP\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c13d4-6f7e-4d9a-9848-6114897b3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reminder: \n",
    "## This process can (and should be!) be done by \n",
    "## calling the model(**batch) to get the lm_head_output directly\n",
    "\n",
    "curr_steps = 0\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    training_loss = 0\n",
    "    validation_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_dl:\n",
    "        curr_steps += 1\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Get the \"input's representation\"\n",
    "        encoder_output = the_encoder(input_ids = batch['input_ids'],\n",
    "                                   attention_mask = batch['attention_mask'])\n",
    "        \n",
    "        # Pass the representation + the target summary to the decoder\n",
    "        decoder_output = the_decoder(input_ids=batch['decoder_input_ids'],\n",
    "                                   attention_mask=batch['decoder_attention_mask'],\n",
    "                                   encoder_hidden_states=encoder_output[0],\n",
    "                                   encoder_attention_mask=batch['attention_mask'])\n",
    "        \n",
    "        # Use the last linear layer to predict the next token\n",
    "        decoder_output = decoder_output.last_hidden_state\n",
    "        lm_head_output = last_linear_layer(decoder_output)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fct(lm_head_output.view(-1, model.config.vocab_size),\n",
    "                      batch['labels'].view(-1))\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "        \n",
    "        wandb.log({\n",
    "          'steps/step': curr_steps,\n",
    "          'steps/epoch': epoch,\n",
    "          'steps/loss': loss.item(),\n",
    "        })\n",
    "        \n",
    "        loss.backward() # Update the weights\n",
    "        optimizer.step() # Notify optimizer that a batch is done.\n",
    "        lr_scheduler.step() # Notify the scheduler that a ...\n",
    "        optimizer.zero_grad() # Reset the optimer\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    for batch in val_dl:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        \n",
    "        validation_loss += loss.item()\n",
    "    \n",
    "    training_loss = training_loss / len(train[\"train\"] )\n",
    "    validation_loss = validation_loss / len(val[\"train\"])\n",
    "    \n",
    "    print(\"Epoch {}:\\tTraining Loss {:.2f}\\t/\\tValidation Loss {:.2f}\".format(epoch+1, training_loss, validation_loss))\n",
    "    \n",
    "    wandb.log({\n",
    "        'epochs/epoch': epoch,\n",
    "        'epochs/train_loss': training_loss,\n",
    "        'epochs/val_loss': validation_loss,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2dfbe-75cc-4c4e-b899-fc9f9c62bb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6105711c-ed4c-410d-aa7e-61fcd75e9aa8",
   "metadata": {},
   "source": [
    "### 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fecde48-4bff-4c9b-9427-d659494de0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{abs_root}/new_mbart_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e8a71-85b7-4bd9-96ae-7449df1efbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp]",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
