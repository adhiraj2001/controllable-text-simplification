{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e408d70f-7c51-4e72-86a4-ceefa7135d96",
   "metadata": {},
   "source": [
    "# Seq2Seq Text Summarization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec7f5bfe-4471-42ea-a650-c06b62d48a71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T14:59:18.350796Z",
     "iopub.status.busy": "2023-12-02T14:59:18.349836Z",
     "iopub.status.idle": "2023-12-02T14:59:23.204809Z",
     "shell.execute_reply": "2023-12-02T14:59:23.203524Z",
     "shell.execute_reply.started": "2023-12-02T14:59:18.350721Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1703215-e73e-4bd7-8b09-a66632dd7953",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T14:59:23.207448Z",
     "iopub.status.busy": "2023-12-02T14:59:23.206899Z",
     "iopub.status.idle": "2023-12-02T14:59:35.767384Z",
     "shell.execute_reply": "2023-12-02T14:59:35.766287Z",
     "shell.execute_reply.started": "2023-12-02T14:59:23.207400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d04c6fd-08b9-4e0a-9472-655ac7460d71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T14:59:35.769901Z",
     "iopub.status.busy": "2023-12-02T14:59:35.769090Z",
     "iopub.status.idle": "2023-12-02T14:59:39.629101Z",
     "shell.execute_reply": "2023-12-02T14:59:39.627993Z",
     "shell.execute_reply.started": "2023-12-02T14:59:35.769844Z"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "# wandb.login(key='913841cb22c908099db4951c258f4242c1d1b7aa')\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_API_KEY'] = '913841cb22c908099db4951c258f4242c1d1b7aa'\n",
    "os.environ['WANDB_SILENT'] = 'true'\n",
    "\n",
    "## To Avoid deadlocks ?\n",
    "# os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c884f7-cf23-43c5-8132-02b1ed28d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_root = '/ssd_scratch/cvit/adhiraj_deshmukh'\n",
    "abs_code = f'{abs_root}/ANLP-Project'\n",
    "abs_data = f'{abs_code}/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a560db2-4623-4ee9-8be0-826f21dc4a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e15a6f1-c826-4914-8cbc-63a3627c25da",
   "metadata": {},
   "source": [
    "## 1. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0483d35-b4a6-4d43-9b9d-d2344fe7d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['source', 'target']\n",
    "\n",
    "train = pd.read_csv(f\"{abs_data}/train_with_parameters.csv\")\n",
    "val = pd.read_csv(f\"{abs_data}/val_with_parameters.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c141246e-9571-406a-92ca-cf87a70779a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(train, test_size=0.2, random_state=42)\n",
    "#train, val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1806e1-38bb-4bc8-99b9-9531761f3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save train, val, test\n",
    "train.to_csv(f'{abs_data}/train_control.csv', index=False)\n",
    "val.to_csv(f'{abs_data}/val_control.csv', index=False)\n",
    "test.to_csv(f'{abs_data}/test_control.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdd022-9a34-4a34-890c-787babe80e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc1e2a8-e50a-47ca-8207-cd9d340b401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# train = load_dataset('csv', data_files=f'{abs_data}/train.csv')\n",
    "# val = load_dataset('csv', data_files=f'{abs_data}/val.csv')\n",
    "# test = load_dataset('csv', data_files=f'{abs_data}/test.csv')\n",
    "\n",
    "train = load_dataset('csv', data_files=f'{abs_data}/train_control.csv')\n",
    "val = load_dataset('csv', data_files=f'{abs_data}/val_control.csv')\n",
    "test = load_dataset('csv', data_files=f'{abs_data}/test_control.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7ede6-8eba-4c62-b3a8-9992e9a5d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"validation\"] = val[\"train\"]\n",
    "train[\"test\"] = test[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cddebbf-986d-4e6c-b003-4504529326f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"train\"] = train[\"train\"].shuffle().select(range(100000))\n",
    "train[\"validation\"] = train[\"validation\"].shuffle().select(range(1000))\n",
    "train[\"test\"] = train[\"test\"].shuffle().select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28192b-fd35-4244-9ec6-4d0aba8cd908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22fb1c58-9abd-4de7-aeed-553b92872ff7",
   "metadata": {},
   "source": [
    "### 2. Tokenize and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc0e060-c6f6-4a82-84ec-1d582f4d5d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the BART's pre-trained Tokenizer\n",
    "from transformers import BartTokenizerFast # 6x Speedup\n",
    "\n",
    "tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-large-cnn', cache_dir=f'{abs_root}/hf_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a529069-e814-40d5-943a-92b23fcdce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_special_tokens({'additional_special_tokens': ['<sep>']})\n",
    "# tokenizer.add_special_tokens({'additional_special_tokens': ['<pad>']})\n",
    "# tokenizer.add_special_tokens({'additional_special_tokens': ['<s>']})\n",
    "# tokenizer.add_special_tokens({'additional_special_tokens': ['</s>']})\n",
    "# tokenizer.add_special_tokens({'additional_special_tokens': ['<unk>']})\n",
    "\n",
    "for i in range(0,11):\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': [f'<copy_{i * 0.1:.1f}>']})\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': [f'<levsim_{i * 0.1:.1f}>']})\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': [f'<cratio_{i * 0.1:.1f}>']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c4f54-b5d8-4cba-a25d-781c0a358dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Summarize:\"\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 64\n",
    "batch_size = 8 # [4, 8, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58e987-c19f-4863-9f7a-04918678da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def clean_text(text):\n",
    "    sentences = nltk.sent_tokenize(text.strip())\n",
    "    sentences_cleaned = [s for sent in sentences for s in sent.split(\"\\n\")]\n",
    "    sentences_cleaned_no_titles = [sent for sent in sentences_cleaned\n",
    "                                 if len(sent) > 0 and\n",
    "                                 sent[-1] in string.punctuation]\n",
    "    \n",
    "    text_cleaned = \"\\n\".join(sentences_cleaned_no_titles)\n",
    "    return text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318a679-91a8-4479-8aee-aba08209e24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd4613-45e6-4794-9c78-91612ca502c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to make the correct data structure\n",
    "def process_data_to_model_inputs(batch):\n",
    "    # inputs = [prefix + f'\\\"{clean_text(text)}\\\"' for text in batch[\"source\"]]\n",
    "\n",
    "    inputs = []\n",
    "    for text, ls, cp, comp in zip(batch[\"source\"], batch[\"lavenstein_similarity\"], batch[\"copy_ratio\"], batch[\"compression_ratio\"]):\n",
    "        inputs.append(f\"<copy_{cp:.1f}> <levsim_{ls:.1f}> <cratio_{comp:.1f}> {prefix} {clean_text(text)}\")\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, padding=\"max_length\", max_length=max_input_length, truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        outputs = [clean_text(text) for text in batch[\"target\"]]\n",
    "        model_outputs = tokenizer(outputs, padding=\"max_length\", max_length=max_target_length, truncation=True)\n",
    "    \n",
    "    batch[\"input_ids\"] = model_inputs.input_ids\n",
    "    batch[\"attention_mask\"] = model_inputs.attention_mask\n",
    "    \n",
    "    batch[\"decoder_input_ids\"] = model_outputs.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = model_outputs.attention_mask\n",
    "    \n",
    "    batch[\"labels\"] = model_outputs.input_ids.copy()\n",
    "    \n",
    "    # We have to make sure that the PAD token is ignored for calculating the loss\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045965b-0479-4e84-b1b5-ebaeeeaf70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the function to both train/validation sets.\n",
    "train = train.map(\n",
    "    process_data_to_model_inputs, \n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"], \n",
    "    batch_size = 1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7de8f4-e03e-4329-9298-281ab35387ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Dataset to PyTorch tensor with the expected columns\n",
    "train.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\",\n",
    "                           \"decoder_attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d671f-b518-42cb-bb06-76cf26b82f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Make the iterative object that does batching using the DataLoader\n",
    "train_dl = DataLoader(train[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(train[\"validation\"], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c97235-d653-4bd7-b317-b0091c2314d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47064b11-5bba-4036-8863-f6e237711999",
   "metadata": {},
   "source": [
    "### 3. Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af0f2c1-c690-49a4-aaff-c4a16ff66bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = BartForConditionalGeneration.from_pretrained(f\"facebook/bart-large-cnn\", cache_dir=f'{abs_root}/hf_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7c450-4fb8-4677-b1b9-0aaf325508ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Incorporate additional tokens\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538a573-27b1-48ac-aaef-b599bbf091f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split model's components\n",
    "# the_encoder = model.get_encoder()\n",
    "# the_decoder = model.get_decoder()\n",
    "\n",
    "# last_linear_layer = model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258cf8e3-87f3-449f-be2f-b25352a961f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze the first n-2 layers\n",
    "# for i in range(len(model.encoder.layers) - 2):\n",
    "#     for param in model.encoder.layers[i].parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# for i in range(len(model.decoder.layers) - 2):\n",
    "#     for param in model.decoder.layers[i].parameters():\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba906795-76d4-4301-b647-3b61b143777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-GPU Batching\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs.\", flush=True)\n",
    "    model = nn.DataParallel(model)\n",
    "    # model = nn.DataParallel(model, device_ids=[2, 3])\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ee8309-5b4e-4748-ba2b-f640cc5ef5f0",
   "metadata": {},
   "source": [
    "### 4. Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e289c92-b208-4e94-bf78-7b479caca000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 10 # [3, 10]\n",
    "num_training_steps = num_epochs * len(train_dl)\n",
    "\n",
    "learning_rate = 1e-3 # [5e-5, 5e-4]\n",
    "lr_scheduler_type = \"linear\" \n",
    "# lr_scheduler_type = \"reduce_lr_on_plateau\"\n",
    "\n",
    "warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "## The loss function\n",
    "loss_fct =  nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "## The optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "lr_scheduler = get_scheduler (\n",
    "    lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    # num_training_steps=num_training_steps\n",
    "    num_training_steps=num_training_steps - warmup_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb629a1-56c4-455d-8fc4-dcfbd436a1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e516341e-d5eb-4468-b43a-80170834a1be",
   "metadata": {},
   "source": [
    "### 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a540c7-ec55-432b-9958-02e806d3bc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"ANLP-Project\",\n",
    "    name=\"bart-large-cnn-controlled (v2)\",\n",
    "    config={\n",
    "        \"architecture\": \"BART\",\n",
    "        \"dataset\": \"Wiki-Auto\",\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        # # \"gamma\": GAMMA,\n",
    "        # # \"step_size\": STEP_SIZE\n",
    "        # \"factor\": FACTOR,\n",
    "        # \"patience\": PATIENCE,\n",
    "        # \"log_step\": LOG_STEP\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c13d4-6f7e-4d9a-9848-6114897b3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reminder: \n",
    "## This process can (and should be!) be done by \n",
    "## calling the model(**batch) to get the lm_head_output directly\n",
    "\n",
    "curr_steps = 0\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    training_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_dl:\n",
    "        curr_steps += 1\n",
    "        \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        \n",
    "        ## Compute the loss\n",
    "        # loss = loss_fct(outputs.view(-1, model.config.vocab_size),\n",
    "        #               batch['labels'].view(-1))\n",
    "        loss = torch.mean(outputs.loss)\n",
    "        training_loss += loss.item()\n",
    "        \n",
    "        wandb.log({\n",
    "          'steps/step': curr_steps,\n",
    "          'steps/epoch': epoch,\n",
    "          'steps/loss': loss.item(),\n",
    "          'steps/lr': float(lr_scheduler.get_last_lr()[0]),\n",
    "        })\n",
    "        \n",
    "        loss.backward() # Update the weights\n",
    "        optimizer.step() # Notify optimizer that a batch is done.\n",
    "        optimizer.zero_grad() # Reset the optimer\n",
    "\n",
    "        # lr_scheduler.step(loss.item()) # Notify the scheduler that a ...\n",
    "        lr_scheduler.step() # Notify the scheduler that a ...\n",
    "\n",
    "    model.eval()\n",
    "    for batch in val_dl:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        \n",
    "        loss = torch.mean(outputs.loss)\n",
    "        \n",
    "        validation_loss += loss.item()\n",
    "    \n",
    "    training_loss = training_loss / len(train[\"train\"] )\n",
    "    validation_loss = validation_loss / len(val[\"train\"])\n",
    "    \n",
    "    print(\"Epoch {}:\\tTraining Loss {:.2f}\\t/\\tValidation Loss {:.2f}\".format(epoch+1, training_loss, validation_loss))\n",
    "    \n",
    "    ## Saving Best model\n",
    "    if best_valid_loss >= validation_loss:\n",
    "        best_valid_loss = validation_loss\n",
    "        best_epoch = epoch + 1\n",
    "        \n",
    "        if hasattr(model, 'module'):\n",
    "            model.module.save_pretrained(f\"{abs_root}/bart-large-cnn-controlled-best\")\n",
    "        else:\n",
    "            model.save_pretrained(f\"{abs_root}/bart-large-cnn-controlled-best\")\n",
    "    \n",
    "    wandb.log({\n",
    "        'epochs/epoch': epoch,\n",
    "        'epochs/train_loss': training_loss,\n",
    "        'epochs/val_loss': validation_loss,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2dfbe-75cc-4c4e-b899-fc9f9c62bb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6105711c-ed4c-410d-aa7e-61fcd75e9aa8",
   "metadata": {},
   "source": [
    "### 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fecde48-4bff-4c9b-9427-d659494de0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(model, 'module'):\n",
    "    model.module.save_pretrained(f\"{abs_root}/bart-large-cnn-controlled-final\")\n",
    "else:\n",
    "    model.save_pretrained(f\"{abs_root}/bart-large-cnn-controlled-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e8a71-85b7-4bd9-96ae-7449df1efbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp]",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
