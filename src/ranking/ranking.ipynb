{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To avoid Cuda out of Memory Error (if doesn't work, try reducing batch size)\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "# from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "\n",
    "    @staticmethod\n",
    "    def gold_score(cand, target, bertscore):\n",
    "        # bs = bertscore.compute(predictions=[cand], references=[target], lang=\"en\", model_type=\"distilbert-base-uncased\")['f1'][0]\n",
    "        _, _, bs = bertscore.score(cands=[cand], refs=[target])\n",
    "        # bs = FeatureExtractor.jaccard_similarity(cand, target)\n",
    "        \n",
    "        len_target = len(word_tokenize(target))\n",
    "        phi_v = len(word_tokenize(cand)) * 1.0 / len_target\n",
    "        phi_y = len(word_tokenize(target)) * 1.0 / len_target\n",
    "        gs = np.exp(abs(phi_v - phi_y)) * bs\n",
    "        return gs\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_candidate(cand, src):\n",
    "        compression_ratio = len(cand) / len(src)\n",
    "        if compression_ratio < 0.5 or compression_ratio > 2.0:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def jaccard_similarity(cand, label) :\n",
    "        cand_set, label_set = set(cand), set(label)\n",
    "        intersection = cand_set & label_set\n",
    "        union = cand_set | label_set\n",
    "        return len(intersection) / len(union)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_feature_vector(cand, src):\n",
    "        features = []\n",
    "        word_tok_src, word_tok_cand = word_tokenize(src.lower()), word_tokenize(cand.lower())\n",
    "        wc_src, wc_cand = len(word_tok_src), len(word_tok_cand)\n",
    "        sc_cand = len(sent_tokenize(cand))\n",
    "        features.append(sc_cand * 1.0)\n",
    "        features.append(wc_src * 1.0)\n",
    "        features.append(FeatureExtractor.jaccard_similarity(word_tok_cand, word_tok_src))\n",
    "        features.append(wc_cand * 1.0 / wc_src)\n",
    "        features.append(wc_cand * 1.0 / sc_cand)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussinaBinner :\n",
    "    def __init__(self, bin_count, gamma) -> None:\n",
    "        self.bin_count = bin_count\n",
    "        self.gamma = gamma\n",
    "        self.means = None\n",
    "        self.sigmas = None\n",
    "\n",
    "    @staticmethod\n",
    "    def gaussian_distance(val, mean, sigma) :\n",
    "        return np.exp(-np.power(val - mean, 2.) / (2 * sigma * sigma))\n",
    "\n",
    "    def create_bins(self, x) :\n",
    "        feature_count = x.shape[1]\n",
    "        self.means = []\n",
    "        self.sigmas = []\n",
    "        for feature in range(feature_count) :\n",
    "            feature_vector = x[:, feature]\n",
    "            feature_min, feature_max = np.min(feature_vector), np.max(feature_vector)\n",
    "            bin_width = (feature_max - feature_min) / self.bin_count\n",
    "            bins = np.arange(self.bin_count + 1) * bin_width + feature_min\n",
    "            mean = np.array([bins[i] + bin_width / 2 for i in range(self.bin_count)])\n",
    "            sigma = bin_width * self.gamma\n",
    "            self.means.append(mean)\n",
    "            self.sigmas.append(sigma)\n",
    "        self.means = np.array(self.means)\n",
    "        self.sigmas = np.array(self.sigmas)\n",
    "    \n",
    "    def generate_vectors(self, x) :\n",
    "        x = np.array(x)\n",
    "        x_reshaped = np.tile(x, (self.bin_count, 1, 1))\n",
    "        means_reshaped = np.tile(self.means.T.reshape(self.bin_count, 1, -1), (1, x.shape[0], 1))\n",
    "        sigmas_reshped = np.tile(self.sigmas, (self.bin_count, x.shape[0], 1))\n",
    "        gaussian = self.gaussian_distance(x_reshaped, means_reshaped, sigmas_reshped)\n",
    "        return np.roll(gaussian, 1, 0).reshape(x.shape[0], -1) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranking_Dataset(Dataset) :\n",
    "    def __init__(self, data, binner, x_path=None, y_path=None) :\n",
    "        # data = json.load(open(data_path, \"r\"))\n",
    "        \n",
    "        if x_path is None and y_path is None :\n",
    "            x = []\n",
    "            y = []\n",
    "            \n",
    "            # self.bert_score = load(\"bertscore\")\n",
    "            self.bert_score = BERTScorer(lang=\"en\", rescale_with_baseline=True, device=device)\n",
    "            \n",
    "            i = 0\n",
    "            for x_i, v_x_i in data.items() :\n",
    "                x.append([])\n",
    "                y.append([])\n",
    "                \n",
    "                i+=1\n",
    "                \n",
    "                # print(f'current_row: {i}')\n",
    "                \n",
    "                for v_x_i_j in v_x_i['cand'] :\n",
    "                    x[-1].append(FeatureExtractor.get_feature_vector(v_x_i_j, x_i))\n",
    "                    y[-1].append(FeatureExtractor.gold_score(v_x_i_j, v_x_i['label'], self.bert_score))\n",
    "            pickle.dump(x, open('x_stored.pkl', \"wb\"))\n",
    "            pickle.dump(y, open('y_stored.pkl', \"wb\"))\n",
    "\n",
    "        else :\n",
    "            x = pickle.load(open(x_path, \"rb\"))\n",
    "            y = pickle.load(open(y_path, \"rb\"))\n",
    "        \n",
    "        self.y = y\n",
    "        self.V = []\n",
    "        \n",
    "        for v_i in x :\n",
    "            self.V.append(binner.generate_vectors(v_i))\n",
    "            \n",
    "        self.n_samples = len(self.V)\n",
    "        self.n_features = 10\n",
    "        \n",
    "#         print(f'y: {len(self.y)}')\n",
    "#         print(f'y[0]: {self.y[0]}')\n",
    "#         print()\n",
    "        \n",
    "#         print(f'V: {len(self.V)}')\n",
    "#         print(f'V[0]: {self.V[0]}')\n",
    "#         print()\n",
    "        \n",
    "\n",
    "    def __len__(self) :\n",
    "        return self.n_samples * self.n_features * self.n_features\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        x_idx = idx // (self.n_features ** 2)\n",
    "        f_idx = idx % (self.n_features ** 2)\n",
    "        \n",
    "        f_i, f_j = f_idx // self.n_features, f_idx % self.n_features\n",
    "        \n",
    "        try:\n",
    "            y = abs(self.y[x_idx][f_i] - self.y[x_idx][f_j])\n",
    "        except Exception as e:\n",
    "            print(f'idx: {idx}')\n",
    "            print(f'n_features: {self.n_features}')\n",
    "            \n",
    "            print(f'x_idx: {x_idx}')\n",
    "            print(f'f_idx: {f_idx}')\n",
    "            \n",
    "            print(f'(f_i, f_j): ({f_i}, {f_j})')\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        return torch.Tensor(self.V[x_idx][f_i]), torch.Tensor(self.V[x_idx][f_j]), torch.Tensor([y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranker:\n",
    "    def __init__(self, binner=None, model=None) -> None:\n",
    "        \n",
    "        self.binner = binner\n",
    "        self.model = model\n",
    "        # self.bertscore = load(\"bertscore\")\n",
    "        \n",
    "        self.make_model(50, 50, 1)\n",
    "        \n",
    "    def make_model(self, in_dim, hidden_dim, out_dim, dropout=0.2) :\n",
    "        self.model = nn.Sequential(\n",
    "            torch.nn.Linear(in_dim, hidden_dim),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Dropout(p=dropout),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Dropout(p=dropout),\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Dropout(p=dropout),\n",
    "            torch.nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        self.model = torch.load(model_path)\n",
    "        return self.model\n",
    "    \n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        torch.save(self.model, model_path)\n",
    "\n",
    "    \n",
    "    def make_binner(self, data) :\n",
    "        x = []\n",
    "        for x_i, v_x_i in data.items() :\n",
    "            for v_x_i_j in v_x_i['cand'] :\n",
    "                x.append(FeatureExtractor.get_feature_vector(v_x_i_j, x_i))\n",
    "        x = np.array(x)\n",
    "        binner = GaussinaBinner(10, 1.0)\n",
    "        binner.create_bins(x)\n",
    "        \n",
    "        self.binner = binner\n",
    "        \n",
    "    def load_data(self, data):\n",
    "        ds = Ranking_Dataset(data, self.binner)\n",
    "        self.dl = DataLoader(ds, batch_size=32, shuffle=False)\n",
    "\n",
    "    def train_model(self, n_epochs, lr) :\n",
    "        \n",
    "        loss_fn = torch.nn.MarginRankingLoss(margin=1.0)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        # if self.binner is None :\n",
    "        #     self.make_binner(data_path)\n",
    "\n",
    "        for epoch in range(n_epochs) :\n",
    "            itr_count = 0\n",
    "            for x_i, x_j, y in self.dl :\n",
    "                itr_count += 1\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                y_i = self.model(x_i)\n",
    "                y_j = self.model(x_j)\n",
    "                \n",
    "                loss = loss_fn(y_i, y_j, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                if itr_count % 1000 == 0: \n",
    "                    print(f\"Epoch: {epoch} ; Itr: {itr_count // 1000} ; Loss: {loss.item()}\")\n",
    "                \n",
    "            print(f\"Epoch {epoch} : Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(\"dict1\", \"rb\"))\n",
    "dict2 = pickle.load(open(\"dict2\", \"rb\"))\n",
    "dict3 = pickle.load(open(\"dict3\", \"rb\"))\n",
    "\n",
    "data.update(dict2)\n",
    "data.update(dict3)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker = Ranker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker.make_binner(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker.load_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker.train_model(5, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anlp]",
   "language": "python",
   "name": "conda-env-anlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
