{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/aparna/miniconda3/envs/diffusion/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "#Encoder-Decoder Model\n",
    "from transformers import EncoderDecoderModel\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Trainer\n",
    "from evaluate import load\n",
    "#Training\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from transformers import  Seq2SeqTrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "bertscore = load(\"bertscore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colnames = ['source', 'target']\n",
    "input_file = \"train.tsv\"\n",
    "df = pd.read_csv(input_file, sep=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8', header=None, names=colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pterocarpus indicus ( commonly known as Amboyn...</td>\n",
       "      <td>Pterocarpus indicus ( commonly known as Amboyn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A romantic friendship , passionate friendship ...</td>\n",
       "      <td>A romantic friendship , passionate friendship ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rocky Balboa is a 2006 American boxing sports ...</td>\n",
       "      <td>Rocky Balboa ( also known as Rocky VI ) is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Janakinath Bose ( 28 May 1860 – 1934 ) was an ...</td>\n",
       "      <td>Janakinath Bose ( 28 May 1860 – 1934 ) was an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bionics or biologically inspired engineering i...</td>\n",
       "      <td>Bionics ( also known as biomimetics ) is the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Pterocarpus indicus ( commonly known as Amboyn...   \n",
       "1  A romantic friendship , passionate friendship ...   \n",
       "2  Rocky Balboa is a 2006 American boxing sports ...   \n",
       "3  Janakinath Bose ( 28 May 1860 – 1934 ) was an ...   \n",
       "4  Bionics or biologically inspired engineering i...   \n",
       "\n",
       "                                              target  \n",
       "0  Pterocarpus indicus ( commonly known as Amboyn...  \n",
       "1  A romantic friendship , passionate friendship ...  \n",
       "2  Rocky Balboa ( also known as Rocky VI ) is the...  \n",
       "3  Janakinath Bose ( 28 May 1860 – 1934 ) was an ...  \n",
       "4  Bionics ( also known as biomimetics ) is the s...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483801\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from #tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<sep>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<pad>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['</s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<unk>']})\n",
    "\n",
    "maxlen = 512\n",
    "def tokenize_df(df):\n",
    "    target = tokenizer(df['target'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input = tokenizer(df['source'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input_ids = input['input_ids']\n",
    "    attention_mask = input['attention_mask']\n",
    "    target_iddatasets import Dataset\n",
    "train_data=Dataset.from_pandas(df[:1000])\n",
    "val_data=Dataset.from_pandas(df[11000:11100])\n",
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "\n",
    "#processing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_length(x):\n",
    "  x[\"source_len\"] = len(tokenizer(x[\"source\"]).input_ids)\n",
    "  x[\"source_128\"] = int(x[\"source_len\"] > 128)\n",
    "  x[\"target_len\"] = len(tokenizer(x[\"target\"]).input_ids)\n",
    "  x[\"target_32\"] = int(x[\"target_len\"] > 32)\n",
    "  x[\"target_64\"] = int(x[\"target_len\"] > 64)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<00:00, 3021.71 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['source', 'target', 'source_len', 'source_128', 'target_len', 'target_32', 'target_64'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sample_size = 1000\n",
    "data_stats = train_data.select(range(sample_size)).map(map_to_length, num_proc=4)\n",
    "print(data_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 205431.94 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Mean: 37.218, %-sources > 128:0.002, Summary Mean:31.608, %-Summary > 32:0.376, %-Summary > 64:0.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_and_print_stats(x):\n",
    "  if len(x[\"source_len\"]) == sample_size:\n",
    "    print(\n",
    "        \"Source Mean: {}, %-sources > 128:{}, Summary Mean:{}, %-Summary > 32:{}, %-Summary > 64:{}\".format(\n",
    "            sum(x[\"source_len\"]) / sample_size,\n",
    "            sum(x[\"source_128\"]) / sample_size, \n",
    "            sum(x[\"target_len\"]) / sample_size,\n",
    "            sum(x[\"target_32\"]) / sample_size,\n",
    "            sum(x[\"target_64\"]) / sample_size,\n",
    "        )\n",
    "    )\n",
    "\n",
    "output = data_stats.map(\n",
    "  compute_and_print_stats, \n",
    "  batched=True,\n",
    "  batch_size=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source1</th>\n",
       "      <th>target1</th>\n",
       "      <th>source2</th>\n",
       "      <th>target2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pterocarpus indicus ( commonly known as Amboyn...</td>\n",
       "      <td>Pterocarpus indicus ( commonly known as Amboyn...</td>\n",
       "      <td>A romantic friendship , passionate friendship ...</td>\n",
       "      <td>A romantic friendship , passionate friendship ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pterocarpus indicus ( commonly known as Amboyn...</td>\n",
       "      <td>Pterocarpus indicus ( commonly known as Amboyn...</td>\n",
       "      <td>Rocky Balboa is a 2006 American boxing sports ...</td>\n",
       "      <td>Rocky Balboa ( also known as Rocky VI ) is the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pterocarpus indicus ( commonly known as Amboyn...</td>\n",
       "      <td>Pterocarpus indicus ( commonly known as Amboyn...</td>\n",
       "      <td>Janakinath Bose ( 28 May 1860 – 1934 ) was an ...</td>\n",
       "      <td>Janakinath Bose ( 28 May 1860 – 1934 ) was an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pterocarpus indicus ( commonly known as Amboyn...</td>\n",
       "      <td>Pterocarpus indicus ( commonly known as Amboyn...</td>\n",
       "      <td>Bionics or biologically inspired engineering i...</td>\n",
       "      <td>Bionics ( also known as biomimetics ) is the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pterocarpus indicus ( commonly known as Amboyn...</td>\n",
       "      <td>Pterocarpus indicus ( commonly known as Amboyn...</td>\n",
       "      <td>The Ilyushin Il-18 ( ; NATO reporting name : C...</td>\n",
       "      <td>Ilyushin Il-18 ( Russian : Илью ́ шин Ил-18 ; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             source1  \\\n",
       "0  Pterocarpus indicus ( commonly known as Amboyn...   \n",
       "1  Pterocarpus indicus ( commonly known as Amboyn...   \n",
       "2  Pterocarpus indicus ( commonly known as Amboyn...   \n",
       "3  Pterocarpus indicus ( commonly known as Amboyn...   \n",
       "4  Pterocarpus indicus ( commonly known as Amboyn...   \n",
       "\n",
       "                                             target1  \\\n",
       "0  Pterocarpus indicus ( commonly known as Amboyn...   \n",
       "1  Pterocarpus indicus ( commonly known as Amboyn...   \n",
       "2  Pterocarpus indicus ( commonly known as Amboyn...   \n",
       "3  Pterocarpus indicus ( commonly known as Amboyn...   \n",
       "4  Pterocarpus indicus ( commonly known as Amboyn...   \n",
       "\n",
       "                                             source2  \\\n",
       "0  A romantic friendship , passionate friendship ...   \n",
       "1  Rocky Balboa is a 2006 American boxing sports ...   \n",
       "2  Janakinath Bose ( 28 May 1860 – 1934 ) was an ...   \n",
       "3  Bionics or biologically inspired engineering i...   \n",
       "4  The Ilyushin Il-18 ( ; NATO reporting name : C...   \n",
       "\n",
       "                                             target2  \n",
       "0  A romantic friendship , passionate friendship ...  \n",
       "1  Rocky Balboa ( also known as Rocky VI ) is the...  \n",
       "2  Janakinath Bose ( 28 May 1860 – 1934 ) was an ...  \n",
       "3  Bionics ( also known as biomimetics ) is the s...  \n",
       "4  Ilyushin Il-18 ( Russian : Илью ́ шин Ил-18 ; ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  #make pairwise data ,paired input and output\n",
    "df = pd.DataFrame()\n",
    "df[\"source\"] = train_data[\"source\"]\n",
    "df[\"target\"] = train_data[\"target\"]\n",
    "#make source1 and source2 and take all possible combinations\n",
    "df1 = df.copy()\n",
    "df2 = df.copy()\n",
    "df1.columns = [\"source1\", \"target1\"]\n",
    "df2.columns = [\"source2\", \"target2\"]\n",
    "df1[\"key\"] = 0\n",
    "df2[\"key\"] = 0\n",
    "df = pd.merge(df1, df2, on=\"key\").drop(\"key\", axis=1)\n",
    "df = df[df[\"source1\"] != df[\"source2\"]]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62438\n"
     ]
    }
   ],
   "source": [
    "train_data = Dataset.from_pandas(df)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "print(len(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.DataFrame()\n",
    "val[\"source\"] = val_data[\"source\"]\n",
    "val[\"target\"] = val_data[\"target\"]\n",
    "val1 = val.copy()\n",
    "val2 = val.copy()\n",
    "val1.columns = [\"source1\", \"target1\"]\n",
    "val2.columns = [\"source2\", \"target2\"]\n",
    "val1[\"key\"] = 0\n",
    "val2[\"key\"] = 0\n",
    "val = pd.merge(val1, val2, on=\"key\").drop(\"key\", axis=1)\n",
    "val = val[val[\"source1\"] != val[\"source2\"]]\n",
    "val = val.reset_index(drop=True)\n",
    "val.head()\n",
    "\n",
    "val_data = Dataset.from_pandas(val)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999000\n",
      "9900\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter setting\n",
    "batch_size=256  #\n",
    "encoder_max_length=128\n",
    "decoder_max_length=64\n",
    "\n",
    "def process_data_to_model_inputs(batch):\n",
    "  # tokenize the inputs and labels\n",
    "  inputs1 = tokenizer(batch[\"source1\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "  outputs1 = tokenizer(batch[\"target1\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "  inputs2 = tokenizer(batch[\"source2\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "  outputs2 = tokenizer(batch[\"target2\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "  batch\n",
    "\n",
    "  batch[\"input_ids1\"] = inputs1.input_ids\n",
    "  batch[\"attention_mask1\"] = inputs1.attention_mask\n",
    "  batch[\"input_ids2\"] = inputs2.input_ids\n",
    "  batch[\"attention_mask2\"] = inputs2.attention_mask\n",
    "  batch[\"decoder_input_ids1\"] = outputs1.input_ids\n",
    "  batch[\"decoder_attention_mask1\"] = outputs1.attention_mask\n",
    "  batch[\"target1\"] = outputs1.input_ids.copy()\n",
    "  batch[\"decoder_input_ids2\"] = outputs2.input_ids\n",
    "  batch[\"decoder_attention_mask2\"] = outputs2.attention_mask\n",
    "  batch[\"target2\"] = outputs2.input_ids.copy()\n",
    "\n",
    "  print(\"input ids1\",batch[\"input_ids1\"])\n",
    "  print(\"input ids2\",batch[\"input_ids2\"])\n",
    "  print(\"decoder_input_ids1\",batch[\"decoder_input_ids1\"])\n",
    "  print(\"decoder_input_ids2\",batch[\"decoder_input_ids2\"])\n",
    "  print(\"target1\",batch[\"target1\"])\n",
    "  print(\"target2\",batch[\"target2\"])\n",
    "  \n",
    "  \n",
    "  \n",
    "  # because RoBERTa automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "  # We have to make sure that the PAD token is ignored\n",
    "  batch[\"target1\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"target1\"]] \n",
    "  batch[\"target2\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"target2\"]]\n",
    "\n",
    "  return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following encoder weights were not tied to the decoder ['roberta/pooler']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import EncoderDecoderModel\n",
    "\n",
    "roberta_shared = EncoderDecoderModel.from_encoder_decoder_pretrained(\"roberta-base\", \"roberta-base\", tie_encoder_decoder=True,cache_dir=\"/scratch/aparna/roberta_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set special tokens\n",
    "roberta_shared.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
    "roberta_shared.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "# set decoding params                               \n",
    "roberta_shared.config.max_length = 100\n",
    "roberta_shared.config.early_stopping = True\n",
    "roberta_shared.config.no_repeat_ngram_size = 3\n",
    "roberta_shared.config.length_penalty = 2.0\n",
    "roberta_shared.config.num_beams = 4\n",
    "roberta_shared.config.vocab_size = roberta_shared.config.encoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12664/1500687920.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = datasets.load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "# load rouge for validation\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    print(pred)\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    \n",
    "        target1 = inputs.pop(\"target1\")\n",
    "        target2 = inputs.pop(\"target2\")\n",
    "        input_ids1 = inputs.pop(\"input_ids1\")\n",
    "        attention_mask1 = inputs.pop(\"attention_mask1\")\n",
    "        input_ids2 = inputs.pop(\"input_ids2\")\n",
    "        attention_mask2 = inputs.pop(\"attention_mask2\")\n",
    "        decoder_input_ids1 = inputs.pop(\"decoder_input_ids1\")\n",
    "        decoder_attention_mask1 = inputs.pop(\"decoder_attention_mask1\")\n",
    "        decoder_input_ids2 = inputs.pop(\"decoder_input_ids2\")\n",
    "        decoder_attention_mask2 = inputs.pop(\"decoder_attention_mask2\")\n",
    "\n",
    "        # forward pass\n",
    "        outputs1 = model(\n",
    "            input_ids=input_ids1,\n",
    "            attention_mask=attention_mask1,\n",
    "            decoder_input_ids=decoder_input_ids1,\n",
    "            decoder_attention_mask=decoder_attention_mask1,\n",
    "            labels=target1,\n",
    "        )\n",
    "        outputs2 = model(\n",
    "            input_ids=input_ids2,\n",
    "            attention_mask=attention_mask2,\n",
    "            decoder_input_ids=decoder_input_ids2,\n",
    "            decoder_attention_mask=decoder_attention_mask2,\n",
    "            labels=target2,\n",
    "        )\n",
    "        \n",
    "        logits1 = outputs1.get(\"logits\")\n",
    "        logits2 = outputs2.get(\"logits\")\n",
    "\n",
    "        #conver to words\n",
    "        pred_ids1 = torch.argmax(logits1, dim=-1)\n",
    "        pred_str1 = tokenizer.batch_decode(pred_ids1, skip_special_tokens=True)\n",
    "        inputs_str1 = tokenizer.batch_decode(inputs[\"input_ids1\"], skip_special_tokens=True)\n",
    "        pred_ids2 = torch.argmax(logits2, dim=-1)\n",
    "        pred_str2 = tokenizer.batch_decode(pred_ids2, skip_special_tokens=True)\n",
    "        inputs_str2 = tokenizer.batch_decode(inputs[\"input_ids2\"], skip_special_tokens=True)\n",
    "\n",
    "        #bert score\n",
    "        F1 = bertscore(pred_str1, inputs_str1, lang=\"en\", verbose=True).f1.mean()\n",
    "        F2 = bertscore(pred_str2, inputs_str2, lang=\"en\", verbose=True).f1.mean()\n",
    "        F1 = torch.tensor(F1)\n",
    "        F2 = torch.tensor(F2)\n",
    "        #length penalty\n",
    "        label_len1 = torch.sum(inputs[\"target1\"] != -100, dim=-1)\n",
    "        input_len1 = torch.sum(inputs[\"input_ids1\"] != tokenizer.pad_token_id, dim=-1)\n",
    "        compression_ratio1 = input_len1 / label_len1\n",
    "\n",
    "        pred_len1 = torch.sum(pred_ids1 != tokenizer.pad_token_id, dim=-1)\n",
    "        pred_ratio1 = pred_len1 / label_len1\n",
    "\n",
    "        label_len2 = torch.sum(inputs[\"target2\"] != -100, dim=-1)\n",
    "        input_len2 = torch.sum(inputs[\"input_ids2\"] != tokenizer.pad_token_id, dim=-1)\n",
    "        compression_ratio2 = input_len2 / label_len2\n",
    "\n",
    "        pred_len2 = torch.sum(pred_ids2 != tokenizer.pad_token_id, dim=-1)\n",
    "        pred_ratio2 = pred_len2 / label_len2\n",
    "\n",
    "        len_penalty1 = torch.exp(-torch.abs(compression_ratio1 - pred_ratio1))\n",
    "        len_penalty2 = torch.exp(-torch.abs(compression_ratio2 - pred_ratio2))\n",
    "\n",
    "        l = torch.abs(len_penalty2*F1 - len_penalty1*F2)\n",
    "        #jacard similarity loss\n",
    "        intersection1 = torch.sum((pred_ids1 == inputs[\"target1\"]) & (inputs[\"target1\"] != -100), dim=-1)\n",
    "        union1 = torch.sum((pred_ids1 != tokenizer.pad_token_id) | (inputs[\"target1\"] != -100), dim=-1)\n",
    "        jacard1 = intersection1 / union1\n",
    "\n",
    "        intersection2 = torch.sum((pred_ids2 == inputs[\"target2\"]) & (inputs[\"target2\"] != -100), dim=-1)\n",
    "        union2 = torch.sum((pred_ids2 != tokenizer.pad_token_id) | (inputs[\"target2\"] != -100), dim=-1)\n",
    "        jacard2 = intersection2 / union2\n",
    "\n",
    "        d = torch.abs(jacard2 - jacard1)\n",
    "        loss = max(0,1 -l*d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return (loss, outputs1,outputs2) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir=\"/scratch/aparna/\",\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     predict_with_generate=True,\n",
    "#     do_train=True,\n",
    "#     do_eval=True,\n",
    "#     logging_steps=2, \n",
    "#     save_steps=16, \n",
    "#     eval_steps=500, \n",
    "#     warmup_steps=500, \n",
    "#     overwrite_output_dir=True,\n",
    "#     save_total_limit=1,\n",
    "#     fp16=True, \n",
    "\n",
    "# )\n",
    "\n",
    "# # instantiate trainer\n",
    "# trainer = CustomTrainer(\n",
    "#     model=roberta_shared,\n",
    "#     args=training_args,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     train_dataset=train_data.map(process_data_to_model_inputs, batched=True),\n",
    "#     eval_dataset=val_data.map(process_data_to_model_inputs, batched=True),\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss( model, inputs, return_outputs=False):\n",
    "    \n",
    "        target1 = inputs.pop(\"target1\")\n",
    "        target2 = inputs.pop(\"target2\")\n",
    "        input_ids1 = inputs.pop(\"input_ids1\")\n",
    "        attention_mask1 = inputs.pop(\"attention_mask1\")\n",
    "        input_ids2 = inputs.pop(\"input_ids2\")\n",
    "        attention_mask2 = inputs.pop(\"attention_mask2\")\n",
    "        decoder_input_ids1 = inputs.pop(\"decoder_input_ids1\")\n",
    "        decoder_attention_mask1 = inputs.pop(\"decoder_attention_mask1\")\n",
    "        decoder_input_ids2 = inputs.pop(\"decoder_input_ids2\")\n",
    "        decoder_attention_mask2 = inputs.pop(\"decoder_attention_mask2\")\n",
    "\n",
    "        # forward pass\n",
    "        outputs1 = model(\n",
    "            input_ids=torch.FloatTensor(input_ids1),\n",
    "            attention_mask=torch.FloatTensor(attention_mask1),\n",
    "            decoder_input_ids=torch.FloatTensor(decoder_input_ids1),\n",
    "            decoder_attention_mask=torch.FloatTensor(decoder_attention_mask1),\n",
    "            labels=torch.FloatTensor(target1),\n",
    "        )\n",
    "        outputs2 = model(\n",
    "            input_ids=input_ids2,\n",
    "            attention_mask=attention_mask2,\n",
    "            decoder_input_ids=decoder_input_ids2,\n",
    "            decoder_attention_mask=decoder_attention_mask2,\n",
    "            labels=target2,\n",
    "        )\n",
    "        \n",
    "        logits1 = outputs1.get(\"logits\")\n",
    "        logits2 = outputs2.get(\"logits\")\n",
    "\n",
    "        #conver to words\n",
    "        pred_ids1 = torch.argmax(logits1, dim=-1)\n",
    "        pred_str1 = tokenizer.batch_decode(pred_ids1, skip_special_tokens=True)\n",
    "        inputs_str1 = tokenizer.batch_decode(inputs[\"input_ids1\"], skip_special_tokens=True)\n",
    "        pred_ids2 = torch.argmax(logits2, dim=-1)\n",
    "        pred_str2 = tokenizer.batch_decode(pred_ids2, skip_special_tokens=True)\n",
    "        inputs_str2 = tokenizer.batch_decode(inputs[\"input_ids2\"], skip_special_tokens=True)\n",
    "\n",
    "        #bert score\n",
    "        F1 = bertscore(pred_str1, inputs_str1, lang=\"en\", verbose=True).f1.mean()\n",
    "        F2 = bertscore(pred_str2, inputs_str2, lang=\"en\", verbose=True).f1.mean()\n",
    "        F1 = torch.tensor(F1)\n",
    "        F2 = torch.tensor(F2)\n",
    "        #length penalty\n",
    "        label_len1 = torch.sum(inputs[\"target1\"] != -100, dim=-1)\n",
    "        input_len1 = torch.sum(inputs[\"input_ids1\"] != tokenizer.pad_token_id, dim=-1)\n",
    "        compression_ratio1 = input_len1 / label_len1\n",
    "\n",
    "        pred_len1 = torch.sum(pred_ids1 != tokenizer.pad_token_id, dim=-1)\n",
    "        pred_ratio1 = pred_len1 / label_len1\n",
    "\n",
    "        label_len2 = torch.sum(inputs[\"target2\"] != -100, dim=-1)\n",
    "        input_len2 = torch.sum(inputs[\"input_ids2\"] != tokenizer.pad_token_id, dim=-1)\n",
    "        compression_ratio2 = input_len2 / label_len2\n",
    "\n",
    "        pred_len2 = torch.sum(pred_ids2 != tokenizer.pad_token_id, dim=-1)\n",
    "        pred_ratio2 = pred_len2 / label_len2\n",
    "\n",
    "        len_penalty1 = torch.exp(-torch.abs(compression_ratio1 - pred_ratio1))\n",
    "        len_penalty2 = torch.exp(-torch.abs(compression_ratio2 - pred_ratio2))\n",
    "\n",
    "        l = torch.abs(len_penalty2*F1 - len_penalty1*F2)\n",
    "        #jacard similarity loss\n",
    "        intersection1 = torch.sum((pred_ids1 == inputs[\"target1\"]) & (inputs[\"target1\"] != -100), dim=-1)\n",
    "        union1 = torch.sum((pred_ids1 != tokenizer.pad_token_id) | (inputs[\"target1\"] != -100), dim=-1)\n",
    "        jacard1 = intersection1 / union1\n",
    "\n",
    "        intersection2 = torch.sum((pred_ids2 == inputs[\"target2\"]) & (inputs[\"target2\"] != -100), dim=-1)\n",
    "        union2 = torch.sum((pred_ids2 != tokenizer.pad_token_id) | (inputs[\"target2\"] != -100), dim=-1)\n",
    "        jacard2 = intersection2 / union2\n",
    "\n",
    "        d = torch.abs(jacard2 - jacard1)\n",
    "        loss = max(0,1 -l*d)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return (loss, outputs1,outputs2) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source1': ['The Zygnemataceae are a family of filamentous or unicellular , uniseriate ( unbranched ) green algae .', 'Murdo John MacKay ( born August 8 , 1917 in Fort William , Ontario - d . August 8 , 2000 ) is a Canadian retired professional ice hockey forward .', 'David IV , also known as David the Builder ( Georgian : , \" \" ) ( 1073 – 24 January 1125 ) , of the Bagrationi dynasty , was a king of Georgia from 1089 until his death in 1125 .', 'Cutout animation is a form of stop-motion animation using flat characters , props and backgrounds cut from materials such as paper , card , stiff fabric or even photographs .', 'BBC Radio 6 Music ( also known as BBC 6 Music or BBC 6 ) is a digital radio station run by the BBC , specialising primarily in alternative music .', 'There are over 100 museums in Seoul .', 'Calvin Klein Inc. is an American luxury fashion house established in 1968 .', 'Robie Creek is a census-designated place in Boise County , Idaho , United States .', 'The Iveco Daily is a large light commercial van produced by the Italian automaker Iveco since 1978 ; it was also sold as the Fiat Daily by Fiat until 1983 .', 'James III ( 10 July 1451 / May 1452 – 11 June 1488 ) was King of Scotland from 1460 to 1488 .', 'The Mil Mi-26 ( , NATO reporting name : Halo ) is a Soviet / Russian heavy transport helicopter .', 'Baharon Ki Manzil is a 1973 Pakistani film directed by S. Suleman .', 'The Tupolev TB-1 ( development name ANT-4 ) was a Soviet bomber aircraft , an angular monoplane that served as the backbone of the Soviet bomber force for many years , and was the first large all-metal aircraft built in the Soviet Union .', 'The Birds is a 1963 American horror-thriller film directed and produced by Alfred Hitchcock .', \"The Ilyushin Il-76 ( ; NATO reporting name : Candid ) is a multi-purpose four-engine turbofan strategic airlifter designed by the Soviet Union 's Ilyushin design bureau .\", 'In mathematics , an automorphism is an isomorphism from a mathematical object to itself .'], 'target1': ['The Zygnematacae are a family of filamentous or unicellular , uniseriate green algae .', 'Murdo John MacKay ( b . 8 August 1917 in Fort William , Ontario - d . August 8 , 2000 ) was a Canadian professional ice hockey forward who played 19 games in the National Hockey League for the Montreal Canadiens .', 'David IV , also known as David II or David III , or David the Builder ( Georgian : დავით აღმაშენებელი , \" Davit Aghmashenebeli \" ) ( 1073 – January 24 , 1125 ) , from the House of Bagrationi , was King of Georgia from 1089 to 1125 .', 'Cutout animation is a method for creating animations using flat characters , props and backgrounds cut from materials such as paper , card , stiff fabric or even photographs .', 'BBC 6 Music is a UK radio station owned by the BBC .', 'This list is about museums in Seoul .', 'Calvin Klein Inc. is an American fashion brand founded in 1968 by Calvin Klein .', 'Robie Creek is a census-designated place ( CDP ) of Idaho in the United States .', 'The Iveco Daily is a series of vans and minibuses produced by Iveco in six generations since 1978 .', 'James III ( c . 1451/1452 - 1488 ) was King of Scotland from 1460 through 1488 .', 'Mil Mi-26 ( Russian : Миль Ми-26 ; NATO reporting name : Halo ) is a Soviet heavy transport helicopter .', 'Baharon Ki Manzil is a 1973 Pakistani movie directed by S. Suleman .', 'The Tupolev TB-1 ( development name ANT-4 ) was a Soviet bomber aircraft produced by Tupolev .', 'The Birds is a 1963 American horror movie directed by Alfred Hitchcock and set in San Francisco , California .', 'Ilyushin Il-76 ( Russian : Ильюшин Ил-76 ; NATO reporting name : Candid ) is a Soviet strategic lifter and cargo aircraft .', 'in abstract algebra an automorphism is an isomorphism from a group , or set of elements onto itself .'], 'source2': ['Autumn is a Dutch female fronted heavy rock band formed in 1995 .', 'The Tabarestan riffle minnow ( \" Alburnoides tabarestanensis \" ) is a species of freshwater fish in the family Cyprinidae .', 'Ina Catani , born Heilborn 30 May 1906 , died 28 March 1938 , was a Swedish archer .', 'Giancarlo Fisichella ( ] ; born 14 January 1973 ) , also known as Fisico , Giano or Fisi , is an Italian professional racing driver .', 'EA Vancouver ( also known as EA Burnaby and formerly known as EA Canada ) is a video game developer located in Burnaby , British Columbia .', 'The ELTons ( English Language Teaching Innovation Awards ) are international awards given annually by the British Council that recognise and celebrate innovation in the field of English language teaching .', 'Ilbe Storage ( Korean : ; RR : \" \" ) , also known as ILBE , is a far-right , right-wing populist website based in South Korea .', 'Automatic train operation ( ATO ) is an operational safety enhancement device used to help automate operations of trains .', 'Jeffrey Donald McDill ( March 16 , 1956 – November 3 , 2012 ) was a Canadian professional ice hockey right winger who played in one National Hockey League game for the Chicago Black Hawks during the 1976 – 77 NHL season .', 'Rockford is a census-designated place in Bingham County , Idaho , United States .', 'Heidelberg is a borough located southwest of Pittsburgh in Allegheny County , Pennsylvania , United States .', 'Classic Shell is a computer software for Microsoft Windows that provides user interface elements intended to restore familiar features from past versions of Windows .', 'Return to Castle Wolfenstein is a first-person shooter video game published by Activision , released on November 19 , 2001 for Microsoft Windows and subsequently for PlayStation 2 , Xbox , Linux and Macintosh .', 'Mr. Freeze : Reverse Blast ( previously known as Mr. Freeze ) is a steel , launched , shuttle roller coaster located at Six Flags Over Texas in Arlington , Texas and Six Flags St. Louis in Eureka , Missouri .', 'In Judaism , a minyan ( Hebrew : מניין \\\\ מִנְיָן \" minya ́ n \" ] , lit .', 'The 380s decade ran from January 1 , 380 , to December 31 , 389 .'], 'target2': ['Autumn is a Dutch gothic metal band formed in 1995 .', 'The Tabarestan riffle minnow ( scientific name : \" Alburnoides tabarestanensis \" ) is a type of freshwater fish that lives in Iran .', 'Ina Catani , born Heilborn 30 May 1906 , dead 28 March 1938 , was a Swedish archer .', 'Giancarlo Fisichella ( born 14 January , 1973 in Rome ) is an Italian driver and drives at present in Formula One with the Renault F1-Team .', 'EA Vancouver ( formerly known as EA Canada and also known as EA Burnaby ) is a video game developer in Burnaby , British Columbia .', 'The ELTon Award is given by the British Council for English language teaching innovation .', 'Ilbe Storage ( 일베저장소 ) , also known Ilbe ( 일베 ) is a website from South Korea .', 'Automatic train operation ( ATO ) ensures partial or complete automatic train piloting and driverless functions .', \"Jeffrey Donald ' Jeff ' McDill ( born March 16 , 1956 ) is a Canadian retired professional ice hockey right winger who played in one National Hockey League game for the Chicago Black Hawks during the 1976 – 77 NHL season .\", 'Rockford is a census-designated place ( CDP ) of Idaho in the United States .', 'Heidelberg is a borough in Allegheny County , Pennsylvania , United States .', 'Classic Shell is a discontinued computer program , that was created to restore Windows XP and Windows Vista features for later versions of Windows .', 'Return to Castle Wolfenstein is a first person shooter computer game made by id Software and published by Activision for computers as well as PlayStation 2 and Xbox game consoles .', 'Mr. Freeze : Reverse Blast ( previously known as Mr. Freeze ) is a steel launched shuttle roller coaster .', 'Minyan , in Judaism , is a group of ten Jewish men ( or women in non Orthodox groups ) that are needed to perform certain rituals .', 'Below are some of the important events that happened between 380 and 389 .']}\n",
      "input ids1 [[0, 133, 30064, 16993, 991, 415, 47334, 32, 10, 284, 9, 46132, 1827, 50, 542, 2463, 45357, 2156, 542, 5999, 10599, 36, 542, 3809, 3290, 4183, 4839, 2272, 25237, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 448, 6831, 139, 610, 1775, 28421, 36, 2421, 830, 290, 2156, 27732, 11, 3339, 2897, 2156, 4170, 111, 385, 479, 830, 290, 2156, 3788, 4839, 16, 10, 1563, 3562, 2038, 2480, 5006, 556, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 8773, 10831, 2156, 67, 684, 25, 871, 5, 40916, 36, 28356, 4832, 2156, 22, 22, 4839, 36, 158, 5352, 126, 706, 644, 112, 11338, 4839, 2156, 9, 5, 13379, 8475, 118, 27284, 2156, 21, 10, 8453, 9, 3090, 31, 158, 5046, 454, 39, 744, 11, 112, 11338, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 40992, 995, 19269, 16, 10, 1026, 9, 912, 12, 35474, 19269, 634, 3269, 3768, 2156, 26504, 8, 14218, 847, 31, 3183, 215, 25, 2225, 2156, 1886, 2156, 13116, 10199, 50, 190, 9065, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 28713, 4611, 231, 3920, 36, 67, 684, 25, 3295, 231, 3920, 50, 3295, 231, 4839, 16, 10, 1778, 3188, 1992, 422, 30, 5, 3295, 2156, 780, 3009, 4212, 11, 3626, 930, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 970, 32, 81, 727, 19773, 11, 9176, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 15117, 6320, 13730, 603, 4, 16, 41, 470, 4808, 2734, 790, 2885, 11, 13466, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 18776, 324, 5099, 16, 10, 18023, 12, 23414, 1070, 317, 11, 20871, 413, 2156, 10604, 2156, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 38, 548, 876, 1681, 16, 10, 739, 1109, 1861, 3538, 2622, 30, 5, 3108, 10948, 4218, 38, 548, 876, 187, 14428, 25606, 24, 21, 67, 1088, 25, 5, 17160, 1681, 30, 17160, 454, 13668, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 18031, 6395, 36, 158, 550, 501, 4708, 1589, 392, 501, 4429, 126, 365, 502, 501, 4652, 4839, 21, 1745, 9, 3430, 31, 501, 2466, 7, 501, 4652, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 5366, 9109, 12, 2481, 36, 2156, 6169, 2207, 766, 4832, 33587, 4839, 16, 10, 8297, 1589, 1083, 2016, 4240, 7324, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 43296, 9303, 11488, 34608, 718, 16, 10, 14757, 9246, 822, 3660, 30, 208, 4, 3296, 459, 397, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 28998, 4104, 705, 19009, 12, 134, 36, 709, 766, 5102, 565, 12, 306, 4839, 21, 10, 8297, 15731, 3054, 2156, 41, 42970, 46547, 21765, 14, 1665, 25, 5, 24456, 9, 5, 8297, 15731, 1370, 13, 171, 107, 2156, 8, 21, 5, 78, 739, 70, 12, 32961, 3054, 1490, 11, 5, 8297, 1332, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 25724, 16, 10, 18733, 470, 8444, 12, 212, 338, 8690, 822, 3660, 8, 2622, 30, 15437, 31713, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 33612, 3810, 179, 8485, 12, 5067, 36, 25606, 6169, 2207, 766, 4832, 11323, 808, 4839, 16, 10, 3228, 12, 25064, 237, 12, 23403, 33858, 1116, 260, 3461, 935, 462, 33211, 1887, 30, 5, 8297, 1332, 128, 29, 33612, 3810, 179, 1521, 12331, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1121, 25634, 2156, 41, 10948, 31724, 1809, 16, 41, 16, 45222, 1809, 31, 10, 30412, 7626, 7, 1495, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "input ids2 [[0, 37434, 24422, 16, 10, 5979, 2182, 760, 196, 2016, 3152, 1971, 4829, 11, 7969, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 12765, 18759, 260, 31457, 459, 5251, 8310, 36, 22, 726, 7554, 139, 4376, 12207, 18759, 260, 45353, 22, 4839, 16, 10, 4707, 9, 34769, 3539, 11, 5, 284, 9384, 4862, 179, 46780, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1121, 102, 7641, 1543, 2156, 2421, 91, 718, 5400, 389, 392, 40947, 2156, 962, 971, 494, 30775, 2156, 21, 10, 9004, 4709, 5260, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 534, 811, 5901, 4082, 274, 354, 11529, 5104, 36, 27779, 25606, 2421, 501, 644, 14757, 4839, 2156, 67, 684, 25, 274, 354, 2684, 2156, 272, 5472, 50, 274, 8539, 2156, 16, 41, 3108, 2038, 4930, 1393, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 14684, 5071, 36, 67, 684, 25, 23506, 7960, 14268, 8, 9598, 684, 25, 23506, 896, 4839, 16, 10, 569, 177, 6596, 2034, 11, 7960, 14268, 2156, 1089, 4635, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 17678, 565, 1790, 36, 2370, 22205, 27987, 12469, 4229, 4839, 32, 758, 4188, 576, 6333, 30, 5, 1089, 1080, 14, 11865, 8, 3379, 4695, 11, 5, 882, 9, 2370, 2777, 5307, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 13693, 1610, 18807, 36, 2238, 4832, 25606, 28440, 4832, 22, 22, 4839, 2156, 67, 684, 25, 11935, 8827, 2156, 16, 10, 444, 12, 4070, 2156, 235, 12, 5577, 16144, 998, 716, 11, 391, 1101, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 37434, 29177, 2341, 2513, 36, 3263, 673, 4839, 16, 41, 5903, 1078, 25387, 2187, 341, 7, 244, 31399, 1414, 9, 7717, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 19663, 5460, 807, 8175, 1873, 36, 494, 545, 2156, 24649, 126, 759, 155, 2156, 1125, 4839, 21, 10, 1563, 2038, 2480, 5006, 235, 10331, 54, 702, 11, 65, 496, 8471, 815, 177, 13, 5, 1568, 1378, 10506, 148, 5, 14488, 126, 6791, 4693, 191, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 10448, 1891, 16, 10, 18023, 12, 23414, 1070, 317, 11, 14365, 1908, 413, 2156, 10604, 2156, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 894, 22088, 2865, 16, 10, 14892, 2034, 10103, 9, 4386, 11, 12447, 27168, 413, 2156, 4367, 2156, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 44950, 10700, 16, 10, 3034, 2257, 13, 3709, 6039, 14, 1639, 3018, 12332, 4785, 3833, 7, 7057, 2950, 1575, 31, 375, 7952, 9, 6039, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42555, 7, 8834, 7602, 27574, 16, 10, 78, 12, 5970, 7846, 569, 177, 1027, 30, 33673, 2156, 703, 15, 759, 753, 2156, 5155, 13, 3709, 6039, 8, 8960, 13, 15592, 132, 2156, 10444, 2156, 15826, 8, 45000, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 10980, 4, 36996, 4832, 41775, 31610, 36, 1433, 684, 25, 427, 4, 36996, 4839, 16, 10, 3689, 2156, 1660, 2156, 19463, 15950, 28910, 2034, 23, 5310, 30552, 2306, 1184, 11, 15728, 2156, 1184, 8, 5310, 30552, 312, 4, 3217, 11, 381, 2407, 2348, 2156, 4630, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1121, 34345, 2156, 10, 5251, 7010, 36, 27428, 4832, 42455, 17772, 42358, 21402, 48177, 46774, 4333, 44128, 42455, 17772, 46030, 20024, 42358, 21402, 46030, 7487, 48177, 46030, 18537, 42358, 4333, 22, 5251, 2636, 1437, 44025, 10172, 295, 22, 27779, 2156, 6474, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 27264, 29, 2202, 2075, 31, 644, 112, 2156, 27264, 2156, 7, 719, 1105, 2156, 43281, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "decoder_input_ids1 [[0, 133, 30064, 16993, 991, 415, 1043, 4791, 32, 10, 284, 9, 46132, 1827, 50, 542, 2463, 45357, 2156, 542, 5999, 10599, 2272, 25237, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 448, 6831, 139, 610, 1775, 28421, 36, 741, 479, 290, 830, 27732, 11, 3339, 2897, 2156, 4170, 111, 385, 479, 830, 290, 2156, 3788, 4839, 21, 10, 1563, 2038, 2480, 5006, 556, 54, 702, 753, 426, 11, 5, 496, 8471, 815, 13, 5, 5817, 18761, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 8773, 10831, 2156, 67, 684, 25, 871, 3082, 50, 871, 6395, 2156, 50, 871, 5, 40916, 36, 28356, 4832, 25974, 862, 9085, 1376, 862, 16948, 1376, 862, 15722, 1376, 862, 711, 1376, 862, 6800, 25974, 862, 16948, 1376, 862, 18164, 1376, 862, 3726, 1376, 862, 16948, 1376, 862, 11423, 1376, 862, 10674, 1376, 862, 48, 1376, 862, 10674, 1376, 862, 3602, 1376, 2], [0, 40992, 995, 19269, 16, 10, 5448, 13, 2351, 39952, 634, 3269, 3768, 2156, 26504, 8, 14218, 847, 31, 3183, 215, 25, 2225, 2156, 1886, 2156, 13116, 10199, 50, 190, 9065, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 28713, 231, 3920, 16, 10, 987, 3188, 1992, 2164, 30, 5, 3295, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 713, 889, 16, 59, 19773, 11, 9176, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 15117, 6320, 13730, 603, 4, 16, 41, 470, 2734, 1518, 4790, 11, 13466, 30, 17079, 13730, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 18776, 324, 5099, 16, 10, 18023, 12, 23414, 1070, 317, 36, 230, 5174, 4839, 9, 10604, 11, 5, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 38, 548, 876, 1681, 16, 10, 651, 9, 26148, 8, 5251, 1452, 9764, 2622, 30, 38, 548, 876, 11, 411, 6808, 187, 14428, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 18031, 6395, 36, 740, 479, 501, 4708, 73, 1570, 4429, 111, 501, 4652, 4839, 21, 1745, 9, 3430, 31, 501, 2466, 149, 501, 4652, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 28151, 9109, 12, 2481, 36, 1083, 4832, 18697, 48, 35328, 40966, 47015, 18697, 48, 35328, 12, 2481, 25606, 6169, 2207, 766, 4832, 33587, 4839, 16, 10, 8297, 2016, 4240, 7324, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 43296, 9303, 11488, 34608, 718, 16, 10, 14757, 9246, 1569, 3660, 30, 208, 4, 3296, 459, 397, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 28998, 4104, 705, 19009, 12, 134, 36, 709, 766, 5102, 565, 12, 306, 4839, 21, 10, 8297, 15731, 3054, 2622, 30, 28998, 4104, 705, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 25724, 16, 10, 18733, 470, 8444, 1569, 3660, 30, 15437, 31713, 8, 278, 11, 764, 2659, 2156, 886, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 100, 352, 3810, 179, 8485, 12, 5067, 36, 1083, 4832, 18697, 711, 40966, 47015, 22063, 12736, 22063, 23133, 35328, 36765, 18697, 711, 40966, 12, 5067, 25606, 6169, 2207, 766, 4832, 11323, 808, 4839, 16, 10, 8297, 3461, 14241, 1334, 8, 9145, 3054, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 179, 20372, 45429, 41, 10948, 31724, 1809, 16, 41, 16, 45222, 1809, 31, 10, 333, 2156, 50, 278, 9, 4785, 2500, 1495, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "decoder_input_ids2 [[0, 37434, 24422, 16, 10, 5979, 821, 6157, 636, 4204, 1971, 4829, 11, 7969, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 12765, 18759, 260, 31457, 459, 5251, 8310, 36, 6441, 766, 4832, 22, 726, 7554, 139, 4376, 12207, 18759, 260, 45353, 22, 4839, 16, 10, 1907, 9, 34769, 3539, 14, 1074, 11, 1603, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1121, 102, 7641, 1543, 2156, 2421, 91, 718, 5400, 389, 392, 40947, 2156, 1462, 971, 494, 30775, 2156, 21, 10, 9004, 4709, 5260, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 534, 811, 5901, 4082, 274, 354, 11529, 5104, 36, 2421, 501, 644, 2156, 14757, 11, 8947, 4839, 16, 41, 3108, 1393, 8, 6790, 23, 1455, 11, 10454, 509, 19, 5, 14833, 274, 134, 12, 20158, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 14684, 5071, 36, 9598, 684, 25, 23506, 896, 8, 67, 684, 25, 23506, 7960, 14268, 4839, 16, 10, 569, 177, 6596, 11, 7960, 14268, 2156, 1089, 4635, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 17678, 44100, 3683, 16, 576, 30, 5, 1089, 1080, 13, 2370, 2777, 5307, 4695, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 13693, 1610, 18807, 36, 25974, 11936, 13859, 1376, 5782, 8906, 1376, 27819, 10965, 1376, 11936, 6382, 1376, 5782, 18164, 1376, 11936, 14285, 1376, 5782, 8210, 1376, 11936, 14285, 1376, 5782, 5543, 1376, 27819, 4394, 1376, 11936, 23171, 1376, 5782, 15375, 4839, 2156, 67, 684, 8485, 1610, 36, 25974, 11936, 13859, 1376, 5782, 8906, 1376, 27819, 10965, 1376, 11936, 6382, 1376, 5782, 18164, 2], [0, 37434, 29177, 2341, 2513, 36, 3263, 673, 4839, 14905, 9801, 50, 1498, 8408, 2341, 4792, 154, 8, 1393, 1672, 8047, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 19663, 5460, 807, 128, 2321, 128, 8175, 1873, 36, 2421, 494, 545, 2156, 24649, 4839, 16, 10, 1563, 3562, 2038, 2480, 5006, 235, 10331, 54, 702, 11, 65, 496, 8471, 815, 177, 13, 5, 1568, 1378, 10506, 148, 5, 14488, 126, 6791, 4693, 191, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 10448, 1891, 16, 10, 18023, 12, 23414, 1070, 317, 36, 230, 5174, 4839, 9, 10604, 11, 5, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 894, 22088, 2865, 16, 10, 14892, 11, 12447, 27168, 413, 2156, 4367, 2156, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 44950, 10700, 16, 10, 27251, 3034, 586, 2156, 14, 21, 1412, 7, 7057, 6039, 30065, 8, 6039, 13737, 1575, 13, 423, 7952, 9, 6039, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42555, 7, 8834, 7602, 27574, 16, 10, 78, 621, 7846, 3034, 177, 156, 30, 13561, 11143, 8, 1027, 30, 33673, 13, 7796, 25, 157, 25, 15592, 132, 8, 10444, 177, 25552, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 10980, 4, 36996, 4832, 41775, 31610, 36, 1433, 684, 25, 427, 4, 36996, 4839, 16, 10, 3689, 1660, 19463, 15950, 28910, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 448, 11287, 260, 2156, 11, 34345, 2156, 16, 10, 333, 9, 2724, 4586, 604, 36, 50, 390, 11, 786, 18466, 1134, 4839, 14, 32, 956, 7, 3008, 1402, 27830, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 45943, 32, 103, 9, 5, 505, 1061, 14, 1102, 227, 27264, 8, 43281, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "target1 [[0, 133, 30064, 16993, 991, 415, 1043, 4791, 32, 10, 284, 9, 46132, 1827, 50, 542, 2463, 45357, 2156, 542, 5999, 10599, 2272, 25237, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 448, 6831, 139, 610, 1775, 28421, 36, 741, 479, 290, 830, 27732, 11, 3339, 2897, 2156, 4170, 111, 385, 479, 830, 290, 2156, 3788, 4839, 21, 10, 1563, 2038, 2480, 5006, 556, 54, 702, 753, 426, 11, 5, 496, 8471, 815, 13, 5, 5817, 18761, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 8773, 10831, 2156, 67, 684, 25, 871, 3082, 50, 871, 6395, 2156, 50, 871, 5, 40916, 36, 28356, 4832, 25974, 862, 9085, 1376, 862, 16948, 1376, 862, 15722, 1376, 862, 711, 1376, 862, 6800, 25974, 862, 16948, 1376, 862, 18164, 1376, 862, 3726, 1376, 862, 16948, 1376, 862, 11423, 1376, 862, 10674, 1376, 862, 48, 1376, 862, 10674, 1376, 862, 3602, 1376, 2], [0, 40992, 995, 19269, 16, 10, 5448, 13, 2351, 39952, 634, 3269, 3768, 2156, 26504, 8, 14218, 847, 31, 3183, 215, 25, 2225, 2156, 1886, 2156, 13116, 10199, 50, 190, 9065, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 28713, 231, 3920, 16, 10, 987, 3188, 1992, 2164, 30, 5, 3295, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 713, 889, 16, 59, 19773, 11, 9176, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 15117, 6320, 13730, 603, 4, 16, 41, 470, 2734, 1518, 4790, 11, 13466, 30, 17079, 13730, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 18776, 324, 5099, 16, 10, 18023, 12, 23414, 1070, 317, 36, 230, 5174, 4839, 9, 10604, 11, 5, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 38, 548, 876, 1681, 16, 10, 651, 9, 26148, 8, 5251, 1452, 9764, 2622, 30, 38, 548, 876, 11, 411, 6808, 187, 14428, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 18031, 6395, 36, 740, 479, 501, 4708, 73, 1570, 4429, 111, 501, 4652, 4839, 21, 1745, 9, 3430, 31, 501, 2466, 149, 501, 4652, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 28151, 9109, 12, 2481, 36, 1083, 4832, 18697, 48, 35328, 40966, 47015, 18697, 48, 35328, 12, 2481, 25606, 6169, 2207, 766, 4832, 33587, 4839, 16, 10, 8297, 2016, 4240, 7324, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 43296, 9303, 11488, 34608, 718, 16, 10, 14757, 9246, 1569, 3660, 30, 208, 4, 3296, 459, 397, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 28998, 4104, 705, 19009, 12, 134, 36, 709, 766, 5102, 565, 12, 306, 4839, 21, 10, 8297, 15731, 3054, 2622, 30, 28998, 4104, 705, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 25724, 16, 10, 18733, 470, 8444, 1569, 3660, 30, 15437, 31713, 8, 278, 11, 764, 2659, 2156, 886, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 100, 352, 3810, 179, 8485, 12, 5067, 36, 1083, 4832, 18697, 711, 40966, 47015, 22063, 12736, 22063, 23133, 35328, 36765, 18697, 711, 40966, 12, 5067, 25606, 6169, 2207, 766, 4832, 11323, 808, 4839, 16, 10, 8297, 3461, 14241, 1334, 8, 9145, 3054, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 179, 20372, 45429, 41, 10948, 31724, 1809, 16, 41, 16, 45222, 1809, 31, 10, 333, 2156, 50, 278, 9, 4785, 2500, 1495, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "target2 [[0, 37434, 24422, 16, 10, 5979, 821, 6157, 636, 4204, 1971, 4829, 11, 7969, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 12765, 18759, 260, 31457, 459, 5251, 8310, 36, 6441, 766, 4832, 22, 726, 7554, 139, 4376, 12207, 18759, 260, 45353, 22, 4839, 16, 10, 1907, 9, 34769, 3539, 14, 1074, 11, 1603, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1121, 102, 7641, 1543, 2156, 2421, 91, 718, 5400, 389, 392, 40947, 2156, 1462, 971, 494, 30775, 2156, 21, 10, 9004, 4709, 5260, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 534, 811, 5901, 4082, 274, 354, 11529, 5104, 36, 2421, 501, 644, 2156, 14757, 11, 8947, 4839, 16, 41, 3108, 1393, 8, 6790, 23, 1455, 11, 10454, 509, 19, 5, 14833, 274, 134, 12, 20158, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 14684, 5071, 36, 9598, 684, 25, 23506, 896, 8, 67, 684, 25, 23506, 7960, 14268, 4839, 16, 10, 569, 177, 6596, 11, 7960, 14268, 2156, 1089, 4635, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 17678, 44100, 3683, 16, 576, 30, 5, 1089, 1080, 13, 2370, 2777, 5307, 4695, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 13693, 1610, 18807, 36, 25974, 11936, 13859, 1376, 5782, 8906, 1376, 27819, 10965, 1376, 11936, 6382, 1376, 5782, 18164, 1376, 11936, 14285, 1376, 5782, 8210, 1376, 11936, 14285, 1376, 5782, 5543, 1376, 27819, 4394, 1376, 11936, 23171, 1376, 5782, 15375, 4839, 2156, 67, 684, 8485, 1610, 36, 25974, 11936, 13859, 1376, 5782, 8906, 1376, 27819, 10965, 1376, 11936, 6382, 1376, 5782, 18164, 2], [0, 37434, 29177, 2341, 2513, 36, 3263, 673, 4839, 14905, 9801, 50, 1498, 8408, 2341, 4792, 154, 8, 1393, 1672, 8047, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 19663, 5460, 807, 128, 2321, 128, 8175, 1873, 36, 2421, 494, 545, 2156, 24649, 4839, 16, 10, 1563, 3562, 2038, 2480, 5006, 235, 10331, 54, 702, 11, 65, 496, 8471, 815, 177, 13, 5, 1568, 1378, 10506, 148, 5, 14488, 126, 6791, 4693, 191, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 10448, 1891, 16, 10, 18023, 12, 23414, 1070, 317, 36, 230, 5174, 4839, 9, 10604, 11, 5, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 894, 22088, 2865, 16, 10, 14892, 11, 12447, 27168, 413, 2156, 4367, 2156, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 44950, 10700, 16, 10, 27251, 3034, 586, 2156, 14, 21, 1412, 7, 7057, 6039, 30065, 8, 6039, 13737, 1575, 13, 423, 7952, 9, 6039, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42555, 7, 8834, 7602, 27574, 16, 10, 78, 621, 7846, 3034, 177, 156, 30, 13561, 11143, 8, 1027, 30, 33673, 13, 7796, 25, 157, 25, 15592, 132, 8, 10444, 177, 25552, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 10980, 4, 36996, 4832, 41775, 31610, 36, 1433, 684, 25, 427, 4, 36996, 4839, 16, 10, 3689, 1660, 19463, 15950, 28910, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 448, 11287, 260, 2156, 11, 34345, 2156, 16, 10, 333, 9, 2724, 4586, 604, 36, 50, 390, 11, 786, 18466, 1134, 4839, 14, 32, 956, 7, 3008, 1402, 27830, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 45943, 32, 103, 9, 5, 505, 1061, 14, 1102, 227, 27264, 8, 43281, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "{'source1': ['The Zygnemataceae are a family of filamentous or unicellular , uniseriate ( unbranched ) green algae .', 'Murdo John MacKay ( born August 8 , 1917 in Fort William , Ontario - d . August 8 , 2000 ) is a Canadian retired professional ice hockey forward .', 'David IV , also known as David the Builder ( Georgian : , \" \" ) ( 1073 – 24 January 1125 ) , of the Bagrationi dynasty , was a king of Georgia from 1089 until his death in 1125 .', 'Cutout animation is a form of stop-motion animation using flat characters , props and backgrounds cut from materials such as paper , card , stiff fabric or even photographs .', 'BBC Radio 6 Music ( also known as BBC 6 Music or BBC 6 ) is a digital radio station run by the BBC , specialising primarily in alternative music .', 'There are over 100 museums in Seoul .', 'Calvin Klein Inc. is an American luxury fashion house established in 1968 .', 'Robie Creek is a census-designated place in Boise County , Idaho , United States .', 'The Iveco Daily is a large light commercial van produced by the Italian automaker Iveco since 1978 ; it was also sold as the Fiat Daily by Fiat until 1983 .', 'James III ( 10 July 1451 / May 1452 – 11 June 1488 ) was King of Scotland from 1460 to 1488 .', 'The Mil Mi-26 ( , NATO reporting name : Halo ) is a Soviet / Russian heavy transport helicopter .', 'Baharon Ki Manzil is a 1973 Pakistani film directed by S. Suleman .', 'The Tupolev TB-1 ( development name ANT-4 ) was a Soviet bomber aircraft , an angular monoplane that served as the backbone of the Soviet bomber force for many years , and was the first large all-metal aircraft built in the Soviet Union .', 'The Birds is a 1963 American horror-thriller film directed and produced by Alfred Hitchcock .', \"The Ilyushin Il-76 ( ; NATO reporting name : Candid ) is a multi-purpose four-engine turbofan strategic airlifter designed by the Soviet Union 's Ilyushin design bureau .\", 'In mathematics , an automorphism is an isomorphism from a mathematical object to itself .'], 'target1': [[0, 133, 30064, 16993, 991, 415, 1043, 4791, 32, 10, 284, 9, 46132, 1827, 50, 542, 2463, 45357, 2156, 542, 5999, 10599, 2272, 25237, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 448, 6831, 139, 610, 1775, 28421, 36, 741, 479, 290, 830, 27732, 11, 3339, 2897, 2156, 4170, 111, 385, 479, 830, 290, 2156, 3788, 4839, 21, 10, 1563, 2038, 2480, 5006, 556, 54, 702, 753, 426, 11, 5, 496, 8471, 815, 13, 5, 5817, 18761, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 8773, 10831, 2156, 67, 684, 25, 871, 3082, 50, 871, 6395, 2156, 50, 871, 5, 40916, 36, 28356, 4832, 25974, 862, 9085, 1376, 862, 16948, 1376, 862, 15722, 1376, 862, 711, 1376, 862, 6800, 25974, 862, 16948, 1376, 862, 18164, 1376, 862, 3726, 1376, 862, 16948, 1376, 862, 11423, 1376, 862, 10674, 1376, 862, 48, 1376, 862, 10674, 1376, 862, 3602, 1376, 2], [0, 40992, 995, 19269, 16, 10, 5448, 13, 2351, 39952, 634, 3269, 3768, 2156, 26504, 8, 14218, 847, 31, 3183, 215, 25, 2225, 2156, 1886, 2156, 13116, 10199, 50, 190, 9065, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 28713, 231, 3920, 16, 10, 987, 3188, 1992, 2164, 30, 5, 3295, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 713, 889, 16, 59, 19773, 11, 9176, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 15117, 6320, 13730, 603, 4, 16, 41, 470, 2734, 1518, 4790, 11, 13466, 30, 17079, 13730, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 18776, 324, 5099, 16, 10, 18023, 12, 23414, 1070, 317, 36, 230, 5174, 4839, 9, 10604, 11, 5, 315, 532, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 133, 38, 548, 876, 1681, 16, 10, 651, 9, 26148, 8, 5251, 1452, 9764, 2622, 30, 38, 548, 876, 11, 411, 6808, 187, 14428, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 18031, 6395, 36, 740, 479, 501, 4708, 73, 1570, 4429, 111, 501, 4652, 4839, 21, 1745, 9, 3430, 31, 501, 2466, 149, 501, 4652, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 28151, 9109, 12, 2481, 36, 1083, 4832, 18697, 48, 35328, 40966, 47015, 18697, 48, 35328, 12, 2481, 25606, 6169, 2207, 766, 4832, 33587, 4839, 16, 10, 8297, 2016, 4240, 7324, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 43296, 9303, 11488, 34608, 718, 16, 10, 14757, 9246, 1569, 3660, 30, 208, 4, 3296, 459, 397, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 133, 28998, 4104, 705, 19009, 12, 134, 36, 709, 766, 5102, 565, 12, 306, 4839, 21, 10, 8297, 15731, 3054, 2622, 30, 28998, 4104, 705, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 133, 25724, 16, 10, 18733, 470, 8444, 1569, 3660, 30, 15437, 31713, 8, 278, 11, 764, 2659, 2156, 886, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 100, 352, 3810, 179, 8485, 12, 5067, 36, 1083, 4832, 18697, 711, 40966, 47015, 22063, 12736, 22063, 23133, 35328, 36765, 18697, 711, 40966, 12, 5067, 25606, 6169, 2207, 766, 4832, 11323, 808, 4839, 16, 10, 8297, 3461, 14241, 1334, 8, 9145, 3054, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 179, 20372, 45429, 41, 10948, 31724, 1809, 16, 41, 16, 45222, 1809, 31, 10, 333, 2156, 50, 278, 9, 4785, 2500, 1495, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]], 'source2': ['Autumn is a Dutch female fronted heavy rock band formed in 1995 .', 'The Tabarestan riffle minnow ( \" Alburnoides tabarestanensis \" ) is a species of freshwater fish in the family Cyprinidae .', 'Ina Catani , born Heilborn 30 May 1906 , died 28 March 1938 , was a Swedish archer .', 'Giancarlo Fisichella ( ] ; born 14 January 1973 ) , also known as Fisico , Giano or Fisi , is an Italian professional racing driver .', 'EA Vancouver ( also known as EA Burnaby and formerly known as EA Canada ) is a video game developer located in Burnaby , British Columbia .', 'The ELTons ( English Language Teaching Innovation Awards ) are international awards given annually by the British Council that recognise and celebrate innovation in the field of English language teaching .', 'Ilbe Storage ( Korean : ; RR : \" \" ) , also known as ILBE , is a far-right , right-wing populist website based in South Korea .', 'Automatic train operation ( ATO ) is an operational safety enhancement device used to help automate operations of trains .', 'Jeffrey Donald McDill ( March 16 , 1956 – November 3 , 2012 ) was a Canadian professional ice hockey right winger who played in one National Hockey League game for the Chicago Black Hawks during the 1976 – 77 NHL season .', 'Rockford is a census-designated place in Bingham County , Idaho , United States .', 'Heidelberg is a borough located southwest of Pittsburgh in Allegheny County , Pennsylvania , United States .', 'Classic Shell is a computer software for Microsoft Windows that provides user interface elements intended to restore familiar features from past versions of Windows .', 'Return to Castle Wolfenstein is a first-person shooter video game published by Activision , released on November 19 , 2001 for Microsoft Windows and subsequently for PlayStation 2 , Xbox , Linux and Macintosh .', 'Mr. Freeze : Reverse Blast ( previously known as Mr. Freeze ) is a steel , launched , shuttle roller coaster located at Six Flags Over Texas in Arlington , Texas and Six Flags St. Louis in Eureka , Missouri .', 'In Judaism , a minyan ( Hebrew : מניין \\\\ מִנְיָן \" minya ́ n \" ] , lit .', 'The 380s decade ran from January 1 , 380 , to December 31 , 389 .'], 'target2': [[0, 37434, 24422, 16, 10, 5979, 821, 6157, 636, 4204, 1971, 4829, 11, 7969, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 133, 12765, 18759, 260, 31457, 459, 5251, 8310, 36, 6441, 766, 4832, 22, 726, 7554, 139, 4376, 12207, 18759, 260, 45353, 22, 4839, 16, 10, 1907, 9, 34769, 3539, 14, 1074, 11, 1603, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 1121, 102, 7641, 1543, 2156, 2421, 91, 718, 5400, 389, 392, 40947, 2156, 1462, 971, 494, 30775, 2156, 21, 10, 9004, 4709, 5260, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 534, 811, 5901, 4082, 274, 354, 11529, 5104, 36, 2421, 501, 644, 2156, 14757, 11, 8947, 4839, 16, 41, 3108, 1393, 8, 6790, 23, 1455, 11, 10454, 509, 19, 5, 14833, 274, 134, 12, 20158, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 14684, 5071, 36, 9598, 684, 25, 23506, 896, 8, 67, 684, 25, 23506, 7960, 14268, 4839, 16, 10, 569, 177, 6596, 11, 7960, 14268, 2156, 1089, 4635, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 133, 17678, 44100, 3683, 16, 576, 30, 5, 1089, 1080, 13, 2370, 2777, 5307, 4695, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 13693, 1610, 18807, 36, 25974, 11936, 13859, 1376, 5782, 8906, 1376, 27819, 10965, 1376, 11936, 6382, 1376, 5782, 18164, 1376, 11936, 14285, 1376, 5782, 8210, 1376, 11936, 14285, 1376, 5782, 5543, 1376, 27819, 4394, 1376, 11936, 23171, 1376, 5782, 15375, 4839, 2156, 67, 684, 8485, 1610, 36, 25974, 11936, 13859, 1376, 5782, 8906, 1376, 27819, 10965, 1376, 11936, 6382, 1376, 5782, 18164, 2], [0, 37434, 29177, 2341, 2513, 36, 3263, 673, 4839, 14905, 9801, 50, 1498, 8408, 2341, 4792, 154, 8, 1393, 1672, 8047, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 19663, 5460, 807, 128, 2321, 128, 8175, 1873, 36, 2421, 494, 545, 2156, 24649, 4839, 16, 10, 1563, 3562, 2038, 2480, 5006, 235, 10331, 54, 702, 11, 65, 496, 8471, 815, 177, 13, 5, 1568, 1378, 10506, 148, 5, 14488, 126, 6791, 4693, 191, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 10448, 1891, 16, 10, 18023, 12, 23414, 1070, 317, 36, 230, 5174, 4839, 9, 10604, 11, 5, 315, 532, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 894, 22088, 2865, 16, 10, 14892, 11, 12447, 27168, 413, 2156, 4367, 2156, 315, 532, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 44950, 10700, 16, 10, 27251, 3034, 586, 2156, 14, 21, 1412, 7, 7057, 6039, 30065, 8, 6039, 13737, 1575, 13, 423, 7952, 9, 6039, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 42555, 7, 8834, 7602, 27574, 16, 10, 78, 621, 7846, 3034, 177, 156, 30, 13561, 11143, 8, 1027, 30, 33673, 13, 7796, 25, 157, 25, 15592, 132, 8, 10444, 177, 25552, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 10980, 4, 36996, 4832, 41775, 31610, 36, 1433, 684, 25, 427, 4, 36996, 4839, 16, 10, 3689, 1660, 19463, 15950, 28910, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 448, 11287, 260, 2156, 11, 34345, 2156, 16, 10, 333, 9, 2724, 4586, 604, 36, 50, 390, 11, 786, 18466, 1134, 4839, 14, 32, 956, 7, 3008, 1402, 27830, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100], [0, 45943, 32, 103, 9, 5, 505, 1061, 14, 1102, 227, 27264, 8, 43281, 479, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]], 'input_ids1': [[0, 133, 30064, 16993, 991, 415, 47334, 32, 10, 284, 9, 46132, 1827, 50, 542, 2463, 45357, 2156, 542, 5999, 10599, 36, 542, 3809, 3290, 4183, 4839, 2272, 25237, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 448, 6831, 139, 610, 1775, 28421, 36, 2421, 830, 290, 2156, 27732, 11, 3339, 2897, 2156, 4170, 111, 385, 479, 830, 290, 2156, 3788, 4839, 16, 10, 1563, 3562, 2038, 2480, 5006, 556, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 8773, 10831, 2156, 67, 684, 25, 871, 5, 40916, 36, 28356, 4832, 2156, 22, 22, 4839, 36, 158, 5352, 126, 706, 644, 112, 11338, 4839, 2156, 9, 5, 13379, 8475, 118, 27284, 2156, 21, 10, 8453, 9, 3090, 31, 158, 5046, 454, 39, 744, 11, 112, 11338, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 40992, 995, 19269, 16, 10, 1026, 9, 912, 12, 35474, 19269, 634, 3269, 3768, 2156, 26504, 8, 14218, 847, 31, 3183, 215, 25, 2225, 2156, 1886, 2156, 13116, 10199, 50, 190, 9065, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 28713, 4611, 231, 3920, 36, 67, 684, 25, 3295, 231, 3920, 50, 3295, 231, 4839, 16, 10, 1778, 3188, 1992, 422, 30, 5, 3295, 2156, 780, 3009, 4212, 11, 3626, 930, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 970, 32, 81, 727, 19773, 11, 9176, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 15117, 6320, 13730, 603, 4, 16, 41, 470, 4808, 2734, 790, 2885, 11, 13466, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 18776, 324, 5099, 16, 10, 18023, 12, 23414, 1070, 317, 11, 20871, 413, 2156, 10604, 2156, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 38, 548, 876, 1681, 16, 10, 739, 1109, 1861, 3538, 2622, 30, 5, 3108, 10948, 4218, 38, 548, 876, 187, 14428, 25606, 24, 21, 67, 1088, 25, 5, 17160, 1681, 30, 17160, 454, 13668, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 18031, 6395, 36, 158, 550, 501, 4708, 1589, 392, 501, 4429, 126, 365, 502, 501, 4652, 4839, 21, 1745, 9, 3430, 31, 501, 2466, 7, 501, 4652, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 5366, 9109, 12, 2481, 36, 2156, 6169, 2207, 766, 4832, 33587, 4839, 16, 10, 8297, 1589, 1083, 2016, 4240, 7324, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 43296, 9303, 11488, 34608, 718, 16, 10, 14757, 9246, 822, 3660, 30, 208, 4, 3296, 459, 397, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 28998, 4104, 705, 19009, 12, 134, 36, 709, 766, 5102, 565, 12, 306, 4839, 21, 10, 8297, 15731, 3054, 2156, 41, 42970, 46547, 21765, 14, 1665, 25, 5, 24456, 9, 5, 8297, 15731, 1370, 13, 171, 107, 2156, 8, 21, 5, 78, 739, 70, 12, 32961, 3054, 1490, 11, 5, 8297, 1332, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 25724, 16, 10, 18733, 470, 8444, 12, 212, 338, 8690, 822, 3660, 8, 2622, 30, 15437, 31713, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 33612, 3810, 179, 8485, 12, 5067, 36, 25606, 6169, 2207, 766, 4832, 11323, 808, 4839, 16, 10, 3228, 12, 25064, 237, 12, 23403, 33858, 1116, 260, 3461, 935, 462, 33211, 1887, 30, 5, 8297, 1332, 128, 29, 33612, 3810, 179, 1521, 12331, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1121, 25634, 2156, 41, 10948, 31724, 1809, 16, 41, 16, 45222, 1809, 31, 10, 30412, 7626, 7, 1495, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask1': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'input_ids2': [[0, 37434, 24422, 16, 10, 5979, 2182, 760, 196, 2016, 3152, 1971, 4829, 11, 7969, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 12765, 18759, 260, 31457, 459, 5251, 8310, 36, 22, 726, 7554, 139, 4376, 12207, 18759, 260, 45353, 22, 4839, 16, 10, 4707, 9, 34769, 3539, 11, 5, 284, 9384, 4862, 179, 46780, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1121, 102, 7641, 1543, 2156, 2421, 91, 718, 5400, 389, 392, 40947, 2156, 962, 971, 494, 30775, 2156, 21, 10, 9004, 4709, 5260, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 534, 811, 5901, 4082, 274, 354, 11529, 5104, 36, 27779, 25606, 2421, 501, 644, 14757, 4839, 2156, 67, 684, 25, 274, 354, 2684, 2156, 272, 5472, 50, 274, 8539, 2156, 16, 41, 3108, 2038, 4930, 1393, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 14684, 5071, 36, 67, 684, 25, 23506, 7960, 14268, 8, 9598, 684, 25, 23506, 896, 4839, 16, 10, 569, 177, 6596, 2034, 11, 7960, 14268, 2156, 1089, 4635, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 17678, 565, 1790, 36, 2370, 22205, 27987, 12469, 4229, 4839, 32, 758, 4188, 576, 6333, 30, 5, 1089, 1080, 14, 11865, 8, 3379, 4695, 11, 5, 882, 9, 2370, 2777, 5307, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 13693, 1610, 18807, 36, 2238, 4832, 25606, 28440, 4832, 22, 22, 4839, 2156, 67, 684, 25, 11935, 8827, 2156, 16, 10, 444, 12, 4070, 2156, 235, 12, 5577, 16144, 998, 716, 11, 391, 1101, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 37434, 29177, 2341, 2513, 36, 3263, 673, 4839, 16, 41, 5903, 1078, 25387, 2187, 341, 7, 244, 31399, 1414, 9, 7717, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 19663, 5460, 807, 8175, 1873, 36, 494, 545, 2156, 24649, 126, 759, 155, 2156, 1125, 4839, 21, 10, 1563, 2038, 2480, 5006, 235, 10331, 54, 702, 11, 65, 496, 8471, 815, 177, 13, 5, 1568, 1378, 10506, 148, 5, 14488, 126, 6791, 4693, 191, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 10448, 1891, 16, 10, 18023, 12, 23414, 1070, 317, 11, 14365, 1908, 413, 2156, 10604, 2156, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 894, 22088, 2865, 16, 10, 14892, 2034, 10103, 9, 4386, 11, 12447, 27168, 413, 2156, 4367, 2156, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 44950, 10700, 16, 10, 3034, 2257, 13, 3709, 6039, 14, 1639, 3018, 12332, 4785, 3833, 7, 7057, 2950, 1575, 31, 375, 7952, 9, 6039, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42555, 7, 8834, 7602, 27574, 16, 10, 78, 12, 5970, 7846, 569, 177, 1027, 30, 33673, 2156, 703, 15, 759, 753, 2156, 5155, 13, 3709, 6039, 8, 8960, 13, 15592, 132, 2156, 10444, 2156, 15826, 8, 45000, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 10980, 4, 36996, 4832, 41775, 31610, 36, 1433, 684, 25, 427, 4, 36996, 4839, 16, 10, 3689, 2156, 1660, 2156, 19463, 15950, 28910, 2034, 23, 5310, 30552, 2306, 1184, 11, 15728, 2156, 1184, 8, 5310, 30552, 312, 4, 3217, 11, 381, 2407, 2348, 2156, 4630, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1121, 34345, 2156, 10, 5251, 7010, 36, 27428, 4832, 42455, 17772, 42358, 21402, 48177, 46774, 4333, 44128, 42455, 17772, 46030, 20024, 42358, 21402, 46030, 7487, 48177, 46030, 18537, 42358, 4333, 22, 5251, 2636, 1437, 44025, 10172, 295, 22, 27779, 2156, 6474, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 27264, 29, 2202, 2075, 31, 644, 112, 2156, 27264, 2156, 7, 719, 1105, 2156, 43281, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask2': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'decoder_input_ids1': [[0, 133, 30064, 16993, 991, 415, 1043, 4791, 32, 10, 284, 9, 46132, 1827, 50, 542, 2463, 45357, 2156, 542, 5999, 10599, 2272, 25237, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 448, 6831, 139, 610, 1775, 28421, 36, 741, 479, 290, 830, 27732, 11, 3339, 2897, 2156, 4170, 111, 385, 479, 830, 290, 2156, 3788, 4839, 21, 10, 1563, 2038, 2480, 5006, 556, 54, 702, 753, 426, 11, 5, 496, 8471, 815, 13, 5, 5817, 18761, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 8773, 10831, 2156, 67, 684, 25, 871, 3082, 50, 871, 6395, 2156, 50, 871, 5, 40916, 36, 28356, 4832, 25974, 862, 9085, 1376, 862, 16948, 1376, 862, 15722, 1376, 862, 711, 1376, 862, 6800, 25974, 862, 16948, 1376, 862, 18164, 1376, 862, 3726, 1376, 862, 16948, 1376, 862, 11423, 1376, 862, 10674, 1376, 862, 48, 1376, 862, 10674, 1376, 862, 3602, 1376, 2], [0, 40992, 995, 19269, 16, 10, 5448, 13, 2351, 39952, 634, 3269, 3768, 2156, 26504, 8, 14218, 847, 31, 3183, 215, 25, 2225, 2156, 1886, 2156, 13116, 10199, 50, 190, 9065, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 28713, 231, 3920, 16, 10, 987, 3188, 1992, 2164, 30, 5, 3295, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 713, 889, 16, 59, 19773, 11, 9176, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 15117, 6320, 13730, 603, 4, 16, 41, 470, 2734, 1518, 4790, 11, 13466, 30, 17079, 13730, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 18776, 324, 5099, 16, 10, 18023, 12, 23414, 1070, 317, 36, 230, 5174, 4839, 9, 10604, 11, 5, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 38, 548, 876, 1681, 16, 10, 651, 9, 26148, 8, 5251, 1452, 9764, 2622, 30, 38, 548, 876, 11, 411, 6808, 187, 14428, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 18031, 6395, 36, 740, 479, 501, 4708, 73, 1570, 4429, 111, 501, 4652, 4839, 21, 1745, 9, 3430, 31, 501, 2466, 149, 501, 4652, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 28151, 9109, 12, 2481, 36, 1083, 4832, 18697, 48, 35328, 40966, 47015, 18697, 48, 35328, 12, 2481, 25606, 6169, 2207, 766, 4832, 33587, 4839, 16, 10, 8297, 2016, 4240, 7324, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 43296, 9303, 11488, 34608, 718, 16, 10, 14757, 9246, 1569, 3660, 30, 208, 4, 3296, 459, 397, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 28998, 4104, 705, 19009, 12, 134, 36, 709, 766, 5102, 565, 12, 306, 4839, 21, 10, 8297, 15731, 3054, 2622, 30, 28998, 4104, 705, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 25724, 16, 10, 18733, 470, 8444, 1569, 3660, 30, 15437, 31713, 8, 278, 11, 764, 2659, 2156, 886, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 100, 352, 3810, 179, 8485, 12, 5067, 36, 1083, 4832, 18697, 711, 40966, 47015, 22063, 12736, 22063, 23133, 35328, 36765, 18697, 711, 40966, 12, 5067, 25606, 6169, 2207, 766, 4832, 11323, 808, 4839, 16, 10, 8297, 3461, 14241, 1334, 8, 9145, 3054, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 179, 20372, 45429, 41, 10948, 31724, 1809, 16, 41, 16, 45222, 1809, 31, 10, 333, 2156, 50, 278, 9, 4785, 2500, 1495, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'decoder_attention_mask1': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'decoder_input_ids2': [[0, 37434, 24422, 16, 10, 5979, 821, 6157, 636, 4204, 1971, 4829, 11, 7969, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 12765, 18759, 260, 31457, 459, 5251, 8310, 36, 6441, 766, 4832, 22, 726, 7554, 139, 4376, 12207, 18759, 260, 45353, 22, 4839, 16, 10, 1907, 9, 34769, 3539, 14, 1074, 11, 1603, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 1121, 102, 7641, 1543, 2156, 2421, 91, 718, 5400, 389, 392, 40947, 2156, 1462, 971, 494, 30775, 2156, 21, 10, 9004, 4709, 5260, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 534, 811, 5901, 4082, 274, 354, 11529, 5104, 36, 2421, 501, 644, 2156, 14757, 11, 8947, 4839, 16, 41, 3108, 1393, 8, 6790, 23, 1455, 11, 10454, 509, 19, 5, 14833, 274, 134, 12, 20158, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 14684, 5071, 36, 9598, 684, 25, 23506, 896, 8, 67, 684, 25, 23506, 7960, 14268, 4839, 16, 10, 569, 177, 6596, 11, 7960, 14268, 2156, 1089, 4635, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 133, 17678, 44100, 3683, 16, 576, 30, 5, 1089, 1080, 13, 2370, 2777, 5307, 4695, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 13693, 1610, 18807, 36, 25974, 11936, 13859, 1376, 5782, 8906, 1376, 27819, 10965, 1376, 11936, 6382, 1376, 5782, 18164, 1376, 11936, 14285, 1376, 5782, 8210, 1376, 11936, 14285, 1376, 5782, 5543, 1376, 27819, 4394, 1376, 11936, 23171, 1376, 5782, 15375, 4839, 2156, 67, 684, 8485, 1610, 36, 25974, 11936, 13859, 1376, 5782, 8906, 1376, 27819, 10965, 1376, 11936, 6382, 1376, 5782, 18164, 2], [0, 37434, 29177, 2341, 2513, 36, 3263, 673, 4839, 14905, 9801, 50, 1498, 8408, 2341, 4792, 154, 8, 1393, 1672, 8047, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 19663, 5460, 807, 128, 2321, 128, 8175, 1873, 36, 2421, 494, 545, 2156, 24649, 4839, 16, 10, 1563, 3562, 2038, 2480, 5006, 235, 10331, 54, 702, 11, 65, 496, 8471, 815, 177, 13, 5, 1568, 1378, 10506, 148, 5, 14488, 126, 6791, 4693, 191, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 10448, 1891, 16, 10, 18023, 12, 23414, 1070, 317, 36, 230, 5174, 4839, 9, 10604, 11, 5, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 894, 22088, 2865, 16, 10, 14892, 11, 12447, 27168, 413, 2156, 4367, 2156, 315, 532, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 44950, 10700, 16, 10, 27251, 3034, 586, 2156, 14, 21, 1412, 7, 7057, 6039, 30065, 8, 6039, 13737, 1575, 13, 423, 7952, 9, 6039, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 42555, 7, 8834, 7602, 27574, 16, 10, 78, 621, 7846, 3034, 177, 156, 30, 13561, 11143, 8, 1027, 30, 33673, 13, 7796, 25, 157, 25, 15592, 132, 8, 10444, 177, 25552, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 10980, 4, 36996, 4832, 41775, 31610, 36, 1433, 684, 25, 427, 4, 36996, 4839, 16, 10, 3689, 1660, 19463, 15950, 28910, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 448, 11287, 260, 2156, 11, 34345, 2156, 16, 10, 333, 9, 2724, 4586, 604, 36, 50, 390, 11, 786, 18466, 1134, 4839, 14, 32, 956, 7, 3008, 1402, 27830, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 45943, 32, 103, 9, 5, 505, 1061, 14, 1102, 227, 27264, 8, 43281, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'decoder_attention_mask2': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home2/aparna/project/train_ranking.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m best_valid_loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(roberta_shared, train_data_loader, optimizer, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(roberta_shared, val_data_loader, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mif\u001b[39;00m valid_loss \u001b[39m<\u001b[39m best_valid_loss:\n",
      "\u001b[1;32m/home2/aparna/project/train_ranking.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m batch \u001b[39m=\u001b[39m process_data_to_model_inputs(batch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(batch)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m compute_loss(model, batch)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/home2/aparna/project/train_ranking.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m decoder_attention_mask2 \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mdecoder_attention_mask2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m outputs1 \u001b[39m=\u001b[39m model(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mFloatTensor(input_ids1),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mFloatTensor(attention_mask1),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mFloatTensor(decoder_input_ids1),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mFloatTensor(decoder_attention_mask1),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     labels\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mFloatTensor(target1),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m outputs2 \u001b[39m=\u001b[39m model(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids2,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask2,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     labels\u001b[39m=\u001b[39mtarget2,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgnode027/home2/aparna/project/train_ranking.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m logits1 \u001b[39m=\u001b[39m outputs1\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:598\u001b[0m, in \u001b[0;36mEncoderDecoderModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m kwargs_decoder \u001b[39m=\u001b[39m {\n\u001b[1;32m    594\u001b[0m     argument[\u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdecoder_\u001b[39m\u001b[39m\"\u001b[39m) :]: value \u001b[39mfor\u001b[39;00m argument, value \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m argument\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mdecoder_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    595\u001b[0m }\n\u001b[1;32m    597\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 598\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    599\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    600\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    601\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    602\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    603\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    604\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    605\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs_encoder,\n\u001b[1;32m    606\u001b[0m     )\n\u001b[1;32m    607\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    608\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\u001b[39m*\u001b[39mencoder_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:837\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    833\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    835\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 837\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    838\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    839\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    840\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    841\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    842\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[1;32m    843\u001b[0m )\n\u001b[1;32m    844\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    845\u001b[0m     embedding_output,\n\u001b[1;32m    846\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    854\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    855\u001b[0m )\n\u001b[1;32m    856\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:125\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    122\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[1;32m    126\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[1;32m    128\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.8/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusion/lib/python3.8/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        print(batch)\n",
    "       # process batch\n",
    "        batch = process_data_to_model_inputs(batch)\n",
    "        print(batch)\n",
    "        loss = compute_loss(model, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Train Loss: {loss.item()}\")\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            batch = process_data_to_model_inputs(batch)\n",
    "            print(batch)\n",
    "            loss = compute_loss(model, batch)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "optimizer = torch.optim.Adam(roberta_shared.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "N_EPOCHS = 10\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(roberta_shared, train_data_loader, optimizer, criterion)\n",
    "    valid_loss = evaluate(roberta_shared, val_data_loader, criterion)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(roberta_shared.state_dict(), '/scratch/aparna/model.pt')\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
