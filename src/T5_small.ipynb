{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "#Training\n",
    "\n",
    "# bertscore = load(\"bertscore\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-small-finetuned-text-simplification\",cache_dir =\"/scratch/aparna\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-small-finetuned-text-simplification\",cache_dir =\"/scratch/aparna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/33801 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 120/33801 [01:42<8:00:37,  1.17it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">21</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_mode.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">27</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 24 │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 25 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 26 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.clone():                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 27 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 28 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> cast(F, decorate_context)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 29 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 30 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_wrap_generator</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, func):                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.p</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1500</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1497 │   │   │   │   </span>output_scores=generation_config.output_scores,                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   │   │   </span>return_dict_in_generate=generation_config.return_dict_in_generate,        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span>synced_gpus=synced_gpus,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1500 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>**model_kwargs,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1501 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> is_beam_sample_gen_mode:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.p</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">y</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2753</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">beam_search</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2750 │   │   │   │   </span>**model_inputs,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2751 │   │   │   │   </span>return_dict=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2752 │   │   │   │   </span>output_attentions=output_attentions,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2753 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output_hidden_states=output_hidden_states,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2754 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2755 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2756 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> synced_gpus <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> this_peer_finished:                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/transformers/models/t5/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_t5.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1716</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1713 │   │   │   </span>use_cache=use_cache,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1714 │   │   │   </span>output_attentions=output_attentions,                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1715 │   │   │   </span>output_hidden_states=output_hidden_states,                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1716 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>return_dict=return_dict,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1717 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1718 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1719 │   │   </span>sequence_output = decoder_outputs[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/transformers/models/t5/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_t5.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1085</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1082 │   │   │   │   │   </span>cross_attn_layer_head_mask=cross_attn_layer_head_mask,                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1083 │   │   │   │   │   </span>past_key_value=past_key_value,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1084 │   │   │   │   │   </span>use_cache=use_cache,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1085 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>output_attentions=output_attentions,                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1086 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1087 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1088 │   │   │   # layer_outputs is a tuple with:</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m21\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/torch/autograd/\u001b[0m\u001b[1;33mgrad_mode.py\u001b[0m:\u001b[94m27\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mdecorate_context\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 24 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 25 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 26 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.clone():                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 27 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 28 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m cast(F, decorate_context)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 29 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 30 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_wrap_generator\u001b[0m(\u001b[96mself\u001b[0m, func):                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.p\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m1500\u001b[0m in \u001b[92mgenerate\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1497 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput_scores=generation_config.output_scores,                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mreturn_dict_in_generate=generation_config.return_dict_in_generate,        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0msynced_gpus=synced_gpus,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1500 \u001b[2m│   │   │   │   \u001b[0m**model_kwargs,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1501 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m is_beam_sample_gen_mode:                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.p\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33my\u001b[0m:\u001b[94m2753\u001b[0m in \u001b[92mbeam_search\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2750 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m**model_inputs,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2751 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mreturn_dict=\u001b[94mTrue\u001b[0m,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2752 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput_attentions=output_attentions,                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2753 \u001b[2m│   │   │   │   \u001b[0moutput_hidden_states=output_hidden_states,                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2754 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2755 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2756 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m synced_gpus \u001b[95mand\u001b[0m this_peer_finished:                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1194\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/transformers/models/t5/\u001b[0m\u001b[1;33mmodeling\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m_t5.py\u001b[0m:\u001b[94m1716\u001b[0m in \u001b[92mforward\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1713 \u001b[0m\u001b[2m│   │   │   \u001b[0muse_cache=use_cache,                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1714 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput_attentions=output_attentions,                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1715 \u001b[0m\u001b[2m│   │   │   \u001b[0moutput_hidden_states=output_hidden_states,                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1716 \u001b[2m│   │   │   \u001b[0mreturn_dict=return_dict,                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1717 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1718 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1719 \u001b[0m\u001b[2m│   │   \u001b[0msequence_output = decoder_outputs[\u001b[94m0\u001b[0m]                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1194\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/transformers/models/t5/\u001b[0m\u001b[1;33mmodeling\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33m_t5.py\u001b[0m:\u001b[94m1085\u001b[0m in \u001b[92mforward\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1082 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mcross_attn_layer_head_mask=cross_attn_layer_head_mask,                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1083 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mpast_key_value=past_key_value,                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1084 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0muse_cache=use_cache,                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1085 \u001b[2m│   │   │   │   │   \u001b[0moutput_attentions=output_attentions,                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1086 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1087 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1088 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# layer_outputs is a tuple with:\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home2/aparna/miniconda3/envs/svoice/lib/python3.7/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1194\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m_call_impl\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#generate\n",
    "\n",
    "\n",
    "max_l = 512\n",
    "num_b = 10\n",
    "num_sub_b =1\n",
    "\n",
    "with open('../data/10/input.txt') as f:\n",
    "    texts = f.readlines()\n",
    "with open('../data/10/labels.txt') as f1:\n",
    "    labels = f1.readlines()\n",
    "dict1 = {}\n",
    "for text in tqdm(texts):\n",
    "    # encode the text into tensor of integers using the appropriate tokenizer\n",
    "    inputs = tokenizer.encode(\"paraphrase: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    dict1[text] = {'cand':[],'label':labels[texts.index(text)]}\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    beam_outputs = model.generate(inputs,max_length=max_l,num_beams=num_b,early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=num_b,\n",
    "        top_k=4, top_p=0.95\n",
    "        # return_dict_in_generate=\n",
    "        #True,\n",
    "    )\n",
    "    # z = json.loads(\"../data/10/output1.json\")\n",
    "    # # appending the data\n",
    "    # z.update(y)\n",
    "\n",
    "\n",
    "    #print(\"====================================\")\n",
    "    for i, beam in enumerate(beam_outputs):\n",
    "        dict1[text]['cand'].append(tokenizer.decode(beam, skip_special_tokens=True))\n",
    "    \n",
    "        #print(\"{}{}\".format(i, tokenizer.decode(beam, skip_special_tokens=True)))\n",
    "# for x in range(10)\n",
    "# with open('../data/10/output1.json', 'w') as fp:\n",
    "#     json.dump(dict1, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "0: Passionate friendship is a close but non-sexual relationship between friends.\n",
      "1: Passionate friendship is a close relationship between friends.\n",
      "2: Passionate friendship is a close but non-sexual relationship between friends and family.\n",
      "3: Passionate friendship is a relationship between friends.\n",
      "4: Passionate friendship is a close but nonsexual relationship between friends.\n",
      "5: Passionate friendship is a close and non-sexual relationship between friends.\n",
      "6: Passionate friendship is a close but non-sexual relationship between friends and friends.\n",
      "7: Passionate friendship is a close friendship between friends.\n",
      "8: Passionate friendship is a relationship between friends and family.\n",
      "9: Passionate friendship is a close relationship between friends and family.\n",
      "====================================\n",
      "0: Electrical switches, fuses or circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "1: Electrical switches, fuses and circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "2: Electrical switches, fuses or circuit breaks were used to control, protect or isolate electrical equipment.\n",
      "3: Electrical switches, fuses or circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "4: Electrical switches, fuses or circuit breakers were used to control, protect and isolate electrical equipment.\n",
      "5: Electric switches, fuses or circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "6: Electrical switches, fuseries or circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "7: Electrical switches, fuses or circuit breakers were used to control electrical equipment.\n",
      "8: Electrical switches, fuses or circuit breakers were used to control and protect electrical equipment.\n",
      "9: Electrical switches were used to control, protect or isolate electrical equipment.\n",
      "====================================\n",
      "0: A hockey league belongs to the Canadian Junior Hockey League.\n",
      "1: A hockey league is part of the Canadian Junior Hockey League.\n",
      "2: A hockey league belongs to the Canadian Junior Hockey League ( CHL ).\n",
      "3: A hockey league belongs to the Canadian Junior Hockey League ( CHL).\n",
      "4: A hockey league belongs to the Canadian Junior Hockey League ( NHL ).\n",
      "5: A hockey league belongs to the Canadian Junior Hockey League ( NHL).\n",
      "6: A hockey league belongs to the Canadian Junior Hockey League.\n",
      "7: A hockey league belongs to the Canadian Junior Hockey League (CHL).\n",
      "8: A hockey league belongs to the Canadian junior hockey league.\n",
      "9: A hockey league belongs to the Canadian Junior Hockey League (CHL ).\n"
     ]
    }
   ],
   "source": [
    "#generate\n",
    "texts= [\"Passionate friendship is a close but non-sexual relationship between friends\",\"Electrical switches , fuses or circuit breakers were used to control , protect or isolate electrical equipment.\",\"A hockey league belongs to the Canadian Junior Hockey League\"]\n",
    "\n",
    "for text in texts:\n",
    "    # encode the text into tensor of integers using the appropriate tokenizer\n",
    "    inputs = tokenizer.encode(\"paraphrase: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    beam_output = model.generate(inputs, max_length=512, \n",
    "        num_beams=10, top_k=120, top_p=0.95, early_stopping=True, num_return_sequences=10)\n",
    "    print(\"====================================\")\n",
    "    for i, beam in enumerate(beam_output):\n",
    "        print(\"{}: {}\".format(i, tokenizer.decode(beam, skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]/tmp/ipykernel_20575/1962435399.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(input_ids).squeeze()\n",
      "/tmp/ipykernel_20575/1962435399.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(attention_mask).squeeze()\n",
      "/tmp/ipykernel_20575/1962435399.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_ids = torch.tensor(target_ids).squeeze()\n",
      "/tmp/ipykernel_20575/1962435399.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_attention_mask = torch.tensor(target_attention_mask).squeeze()\n",
      "Map: 100%|██████████| 20000/20000 [00:11<00:00, 1703.66 examples/s]\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import numpy\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from transformers import pipeline\n",
    "#import train split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from google.transliteration import transliterate_word\n",
    "#import klib\n",
    "import os\n",
    "#bleu score\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import csv\n",
    "colnames = ['source', 'target']\n",
    "input_file = \"train.tsv\"\n",
    "#df = pd.read_csv(input_file, sep=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8', header=None, names=colnames)\n",
    "test = pd.read_csv(\"valid.tsv\", sep=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8', header=None, names=colnames)\n",
    "test = Dataset.from_pandas(test)\n",
    "# %%\n",
    "#tokenize\n",
    "#tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<sep>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<pad>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['</s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<unk>']})\n",
    "\n",
    "maxlen = 512\n",
    "def tokenize_df(df):\n",
    "    target = tokenizer(df['target'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input = tokenizer(df['source'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input_ids = input['input_ids']\n",
    "    attention_mask = input['attention_mask']\n",
    "    target_ids = target['input_ids']\n",
    "    target_attention_mask = target['attention_mask']\n",
    "    decoder_input_ids = target_ids.clone()\n",
    "    #convert to tensors\n",
    "    input_ids = torch.tensor(input_ids).squeeze()\n",
    "    attention_mask = torch.tensor(attention_mask).squeeze()\n",
    "    target_ids = torch.tensor(target_ids).squeeze()\n",
    "    target_attention_mask = torch.tensor(target_attention_mask).squeeze()\n",
    "   # decoder_input_ids = torch.tensor(decoder_input_ids)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': target_ids,\n",
    "        #'decoder_input_ids': decoder_input_ids,\n",
    "        #'decoder_attention_mask': target_attention_mask\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# train = load_dataset('csv', data_files='train.csv')\n",
    "# val = load_dataset('csv', data_files='val.csv')\n",
    "# test = load_dataset('csv', data_files='test.csv')\n",
    "# train = train.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n",
    "# val = val.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n",
    "test = test.map(tokenize_df, batched=True, batch_size=128,remove_columns=['source','target'])\n",
    "\n",
    "\n",
    "# %%\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def tokenize_sentence(arg):\n",
    "    encoded_arg =tokenizer(arg)\n",
    "    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    preds, labels = eval_arg\n",
    "    # Replace -100\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Convert id tokens to text\n",
    "    text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Insert a line break (\\n) in each sentence for ROUGE scoring\n",
    "    # (Note : Please change this code, when you perform on other languages except for Japanese)\n",
    "    text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "    text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "    sent_tokenizer_jp = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n",
    "    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n",
    "    # compute ROUGE score with custom tokenization\n",
    "    #blue score\n",
    "    texts_bleu =[text.strip() for text in text_preds]\n",
    "    labels_bleu = [[text.strip()] for text in text_labels]\n",
    "    result = bleu_metric.compute(predictions=texts_bleu, references=labels_bleu)\n",
    "    return rouge_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        tokenizer=tokenize_sentence\n",
    "    ), result['score']\n",
    "\n",
    "# %%\n",
    "from torch.utils.data import DataLoader\n",
    "#tokenizer = MT5Tokenizer.from_pretrained(\"./mt5\")\n",
    "def testing(model):\n",
    "    metrics =[]\n",
    "    sample_dataloader = DataLoader(\n",
    "      test.with_format(\"torch\"),\n",
    "      collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "      batch_size=5)\n",
    "    for batch in sample_dataloader:\n",
    "      with torch.no_grad():\n",
    "        preds = model.generate(\n",
    "          batch[\"input_ids\"],\n",
    "          num_beams=15,\n",
    "          num_return_sequences=1,\n",
    "          no_repeat_ngram_size=1,\n",
    "          remove_invalid_values=True,\n",
    "          max_length=128,\n",
    "        )\n",
    "      labels = batch[\"labels\"]\n",
    "      metric = metrics_func([preds, labels])\n",
    "      metrics.append(metric)\n",
    "    return metrics\n",
    "\n",
    "def average_metric(metrics):\n",
    "    rouge = 0\n",
    "    rouge2 = 0\n",
    "    rougeL = 0\n",
    "    rougeLsum = 0\n",
    "    bleu = 0\n",
    "    for metric in metrics:\n",
    "        rouge += metric[0]['rouge1']\n",
    "        rouge2 += metric[0]['rouge2']\n",
    "        rougeL += metric[0]['rougeL']\n",
    "        rougeLsum += metric[0]['rougeLsum']\n",
    "        bleu += metric[1]\n",
    "    return rouge/len(metrics),rouge2/len(metrics),rougeL/len(metrics),rougeLsum/len(metrics),bleu/len(metrics)\n",
    "      \n",
    "\n",
    "# %%\n",
    "\n",
    "metrics = testing(model)\n",
    "\n",
    "# %%\n",
    "print(\"t5_small\")\n",
    "scores = average_metric(metrics)\n",
    "print(\"rouge:\",scores[0])\n",
    "print(\"rouge2:\",scores[1])\n",
    "print(\"rougeL:\",scores[2])\n",
    "print(\"rougeLsum:\",scores[3])\n",
    "print(\"bleu:\",scores[4])\n",
    "\n",
    "\n",
    "# %%\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Predict with test data (first 5 rows)\n",
    "sample_dataloader = DataLoader(\n",
    "  test.with_format(\"torch\"),\n",
    "  collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"],\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  inputs = batch[\"input_ids\"]\n",
    "  break\n",
    "\n",
    "# Replace -100 (see above)\n",
    "inputs = np.where(inputs != -100, inputs, tokenizer.pad_token_id)\n",
    "labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "# Convert id tokens to text\n",
    "text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "text_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "#print(bleu_score(list(text_labels.split()),list(text_preds.split())))\n",
    "\n",
    "# Show result\n",
    "print(\"***** Input's Text *****\")\n",
    "print(text_inputs[2])\n",
    "print(\"***** summmary (True Value) *****\")\n",
    "print(text_labels[2])\n",
    "print(\"***** summary (Generated Text) *****\")\n",
    "print(text_preds[2])\n",
    "\n",
    "# %%\n",
    "for i in range(5):\n",
    "    print(\"***** Input's Text *****\")\n",
    "    print(text_inputs[i])\n",
    "    print(\"***** summarry (True Value) *****\")\n",
    "    print(text_labels[i])\n",
    "    print(\"***** summary (Generated Text) *****\")\n",
    "    print(text_preds[i])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
