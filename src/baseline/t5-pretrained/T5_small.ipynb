{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import csv\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "#Encoder-Decoder Model\n",
    "from transformers import EncoderDecoderModel\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Trainer\n",
    "from evaluate import load\n",
    "#Training\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from transformers import  Seq2SeqTrainingArguments\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import \n",
    "# bertscore = load(\"bertscore\") \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-small-finetuned-text-simplification\",cache_dir =\"/scratch/aparna\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-small-finetuned-text-simplification\",cache_dir =\"/scratch/aparna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate\n",
    "\n",
    "for text in texts:\n",
    "    # encode the text into tensor of integers using the appropriate tokenizer\n",
    "    inputs = tokenizer.encode(\"paraphrase: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    beam_output = model.generate(inputs, max_length=512, \n",
    "        num_beams=10, top_k=120, top_p=0.95, early_stopping=True, num_return_sequences=10)\n",
    "    print(\"====================================\")\n",
    "    for i, beam in enumerate(beam_output):\n",
    "        print(\"{}: {}\".format(i, tokenizer.decode(beam, skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "0: Passionate friendship is a close but non-sexual relationship between friends.\n",
      "1: Passionate friendship is a close relationship between friends.\n",
      "2: Passionate friendship is a close but non-sexual relationship between friends and family.\n",
      "3: Passionate friendship is a relationship between friends.\n",
      "4: Passionate friendship is a close but nonsexual relationship between friends.\n",
      "5: Passionate friendship is a close and non-sexual relationship between friends.\n",
      "6: Passionate friendship is a close but non-sexual relationship between friends and friends.\n",
      "7: Passionate friendship is a close friendship between friends.\n",
      "8: Passionate friendship is a relationship between friends and family.\n",
      "9: Passionate friendship is a close relationship between friends and family.\n",
      "====================================\n",
      "0: Electrical switches, fuses or circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "1: Electrical switches, fuses and circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "2: Electrical switches, fuses or circuit breaks were used to control, protect or isolate electrical equipment.\n",
      "3: Electrical switches, fuses or circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "4: Electrical switches, fuses or circuit breakers were used to control, protect and isolate electrical equipment.\n",
      "5: Electric switches, fuses or circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "6: Electrical switches, fuseries or circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "7: Electrical switches, fuses or circuit breakers were used to control electrical equipment.\n",
      "8: Electrical switches, fuses or circuit breakers were used to control and protect electrical equipment.\n",
      "9: Electrical switches were used to control, protect or isolate electrical equipment.\n",
      "====================================\n",
      "0: A hockey league belongs to the Canadian Junior Hockey League.\n",
      "1: A hockey league is part of the Canadian Junior Hockey League.\n",
      "2: A hockey league belongs to the Canadian Junior Hockey League ( CHL ).\n",
      "3: A hockey league belongs to the Canadian Junior Hockey League ( CHL).\n",
      "4: A hockey league belongs to the Canadian Junior Hockey League ( NHL ).\n",
      "5: A hockey league belongs to the Canadian Junior Hockey League ( NHL).\n",
      "6: A hockey league belongs to the Canadian Junior Hockey League.\n",
      "7: A hockey league belongs to the Canadian Junior Hockey League (CHL).\n",
      "8: A hockey league belongs to the Canadian junior hockey league.\n",
      "9: A hockey league belongs to the Canadian Junior Hockey League (CHL ).\n"
     ]
    }
   ],
   "source": [
    "#generate\n",
    "\n",
    "\n",
    "max_l = 512\n",
    "num_b = 10\n",
    "num_sub_b =1\n",
    "\n",
    "with open('data/input.txt') as f:\n",
    "    texts = f.readlines()\n",
    "\n",
    "for text in texts:\n",
    "    # encode the text into tensor of integers using the appropriate tokenizer\n",
    "    inputs = tokenizer.encode(\"paraphrase: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    beam_outputs = model.generate(inputs,max_length=max_l,num_beams=num_b,early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=num_b,\n",
    "        top_k=4, top_p=0.95\n",
    "        # return_dict_in_generate=True,\n",
    "    )\n",
    "    print(\"====================================\")\n",
    "    for i, beam in enumerate(beam_outputs):\n",
    "        print(\"{}\".format(i, tokenizer.decode(beam, skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "0: Passionate friendship is a close but non-sexual relationship between friends.\n",
      "1: Passionate friendship is a close relationship between friends.\n",
      "2: Passionate friendship is a close but non-sexual relationship between friends and family.\n",
      "3: Passionate friendship is a relationship between friends.\n",
      "4: Passionate friendship is a close but nonsexual relationship between friends.\n",
      "5: Passionate friendship is a close and non-sexual relationship between friends.\n",
      "6: Passionate friendship is a close but non-sexual relationship between friends and friends.\n",
      "7: Passionate friendship is a close friendship between friends.\n",
      "8: Passionate friendship is a relationship between friends and family.\n",
      "9: Passionate friendship is a close relationship between friends and family.\n",
      "====================================\n",
      "0: Electrical switches, fuses or circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "1: Electrical switches, fuses and circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "2: Electrical switches, fuses or circuit breaks were used to control, protect or isolate electrical equipment.\n",
      "3: Electrical switches, fuses or circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "4: Electrical switches, fuses or circuit breakers were used to control, protect and isolate electrical equipment.\n",
      "5: Electric switches, fuses or circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "6: Electrical switches, fuseries or circuit breakers were used to control, protect or isolate electrical equipment.\n",
      "7: Electrical switches, fuses or circuit breakers were used to control electrical equipment.\n",
      "8: Electrical switches, fuses or circuit breakers were used to control and protect electrical equipment.\n",
      "9: Electrical switches were used to control, protect or isolate electrical equipment.\n",
      "====================================\n",
      "0: A hockey league belongs to the Canadian Junior Hockey League.\n",
      "1: A hockey league is part of the Canadian Junior Hockey League.\n",
      "2: A hockey league belongs to the Canadian Junior Hockey League ( CHL ).\n",
      "3: A hockey league belongs to the Canadian Junior Hockey League ( CHL).\n",
      "4: A hockey league belongs to the Canadian Junior Hockey League ( NHL ).\n",
      "5: A hockey league belongs to the Canadian Junior Hockey League ( NHL).\n",
      "6: A hockey league belongs to the Canadian Junior Hockey League.\n",
      "7: A hockey league belongs to the Canadian Junior Hockey League (CHL).\n",
      "8: A hockey league belongs to the Canadian junior hockey league.\n",
      "9: A hockey league belongs to the Canadian Junior Hockey League (CHL ).\n"
     ]
    }
   ],
   "source": [
    "#generate\n",
    "texts= [\"Passionate friendship is a close but non-sexual relationship between friends\",\"Electrical switches , fuses or circuit breakers were used to control , protect or isolate electrical equipment.\",\"A hockey league belongs to the Canadian Junior Hockey League\"]\n",
    "\n",
    "max_l = 512\n",
    "num_b = 10\n",
    "num_sub_b =1\n",
    "\n",
    "for text in texts:\n",
    "    # encode the text into tensor of integers using the appropriate tokenizer\n",
    "    inputs = tokenizer.encode(\"paraphrase: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    beam_outputs = model.generate(inputs,max_length=max_l,num_beams=num_b,num_beam_groups=num_sub_b,early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_return_sequences=num_b,\n",
    "        top_k=4, top_p=0.95\n",
    "        # return_dict_in_generate=True,\n",
    "    )\n",
    "    print(\"====================================\")\n",
    "    for i, beam in enumerate(beam_outputs):\n",
    "        print(\"{}: {}\".format(i, tokenizer.decode(beam, skip_special_tokens=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import numpy\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "from transformers import pipeline\n",
    "#import train split\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from google.transliteration import transliterate_word\n",
    "#import klib\n",
    "import os\n",
    "#bleu score\n",
    "from torchtext.data.metrics import bleu_score\n",
    "import csv\n",
    "colnames = ['source', 'target']\n",
    "input_file = \"train.tsv\"\n",
    "#df = pd.read_csv(input_file, sep=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8', header=None, names=colnames)\n",
    "test = pd.read_csv(\"valid.tsv\", sep=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8', header=None, names=colnames)\n",
    "test = Dataset.from_pandas(test)\n",
    "# %%\n",
    "#tokenize\n",
    "#tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<sep>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<pad>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['</s>']})\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<unk>']})\n",
    "\n",
    "maxlen = 512\n",
    "def tokenize_df(df):\n",
    "    target = tokenizer(df['target'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input = tokenizer(df['source'], padding='max_length', truncation=True, return_tensors=\"pt\", max_length=maxlen)\n",
    "    input_ids = input['input_ids']\n",
    "    attention_mask = input['attention_mask']\n",
    "    target_ids = target['input_ids']\n",
    "    target_attention_mask = target['attention_mask']\n",
    "    decoder_input_ids = target_ids.clone()\n",
    "    #convert to tensors\n",
    "    input_ids = torch.tensor(input_ids).squeeze()\n",
    "    attention_mask = torch.tensor(attention_mask).squeeze()\n",
    "    target_ids = torch.tensor(target_ids).squeeze()\n",
    "    target_attention_mask = torch.tensor(target_attention_mask).squeeze()\n",
    "   # decoder_input_ids = torch.tensor(decoder_input_ids)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': target_ids,\n",
    "        #'decoder_input_ids': decoder_input_ids,\n",
    "        #'decoder_attention_mask': target_attention_mask\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# train = load_dataset('csv', data_files='train.csv')\n",
    "# val = load_dataset('csv', data_files='val.csv')\n",
    "# test = load_dataset('csv', data_files='test.csv')\n",
    "# train = train.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n",
    "# val = val.map(tokenize_df, batched=True, batch_size=128,remove_columns=['sentence','english_translation'])\n",
    "test = test.map(tokenize_df, batched=True, batch_size=128,remove_columns=['source','target'])\n",
    "\n",
    "\n",
    "# %%\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def tokenize_sentence(arg):\n",
    "    encoded_arg =tokenizer(arg)\n",
    "    return tokenizer.convert_ids_to_tokens(encoded_arg.input_ids)\n",
    "\n",
    "def metrics_func(eval_arg):\n",
    "    preds, labels = eval_arg\n",
    "    # Replace -100\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Convert id tokens to text\n",
    "    text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Insert a line break (\\n) in each sentence for ROUGE scoring\n",
    "    # (Note : Please change this code, when you perform on other languages except for Japanese)\n",
    "    text_preds = [(p if p.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else p + \"。\") for p in text_preds]\n",
    "    text_labels = [(l if l.endswith((\"!\", \"！\", \"?\", \"？\", \"。\")) else l + \"。\") for l in text_labels]\n",
    "    sent_tokenizer_jp = RegexpTokenizer(u'[^!！?？。]*[!！?？。]')\n",
    "    text_preds = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(p))) for p in text_preds]\n",
    "    text_labels = [\"\\n\".join(np.char.strip(sent_tokenizer_jp.tokenize(l))) for l in text_labels]\n",
    "    # compute ROUGE score with custom tokenization\n",
    "    #blue score\n",
    "    texts_bleu =[text.strip() for text in text_preds]\n",
    "    labels_bleu = [[text.strip()] for text in text_labels]\n",
    "    result = bleu_metric.compute(predictions=texts_bleu, references=labels_bleu)\n",
    "    return rouge_metric.compute(\n",
    "        predictions=text_preds,\n",
    "        references=text_labels,\n",
    "        tokenizer=tokenize_sentence\n",
    "    ), result['score']\n",
    "\n",
    "# %%\n",
    "from torch.utils.data import DataLoader\n",
    "#tokenizer = MT5Tokenizer.from_pretrained(\"./mt5\")\n",
    "def testing(model):\n",
    "    metrics =[]\n",
    "    sample_dataloader = DataLoader(\n",
    "      test.with_format(\"torch\"),\n",
    "      collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "      batch_size=5)\n",
    "    for batch in sample_dataloader:\n",
    "      with torch.no_grad():\n",
    "        preds = model.generate(\n",
    "          batch[\"input_ids\"],\n",
    "          num_beams=15,\n",
    "          num_return_sequences=1,\n",
    "          no_repeat_ngram_size=1,\n",
    "          remove_invalid_values=True,\n",
    "          max_length=128,\n",
    "        )\n",
    "      labels = batch[\"labels\"]\n",
    "      metric = metrics_func([preds, labels])\n",
    "      metrics.append(metric)\n",
    "    return metrics\n",
    "\n",
    "def average_metric(metrics):\n",
    "    rouge = 0\n",
    "    rouge2 = 0\n",
    "    rougeL = 0\n",
    "    rougeLsum = 0\n",
    "    bleu = 0\n",
    "    for metric in metrics:\n",
    "        rouge += metric[0]['rouge1']\n",
    "        rouge2 += metric[0]['rouge2']\n",
    "        rougeL += metric[0]['rougeL']\n",
    "        rougeLsum += metric[0]['rougeLsum']\n",
    "        bleu += metric[1]\n",
    "    return rouge/len(metrics),rouge2/len(metrics),rougeL/len(metrics),rougeLsum/len(metrics),bleu/len(metrics)\n",
    "      \n",
    "\n",
    "# %%\n",
    "\n",
    "metrics = testing(model)\n",
    "\n",
    "# %%\n",
    "print(\"t5_small\")\n",
    "scores = average_metric(metrics)\n",
    "print(\"rouge:\",scores[0])\n",
    "print(\"rouge2:\",scores[1])\n",
    "print(\"rougeL:\",scores[2])\n",
    "print(\"rougeLsum:\",scores[3])\n",
    "print(\"bleu:\",scores[4])\n",
    "\n",
    "\n",
    "# %%\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Predict with test data (first 5 rows)\n",
    "sample_dataloader = DataLoader(\n",
    "  test.with_format(\"torch\"),\n",
    "  collate_fn=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "  batch_size=5)\n",
    "for batch in sample_dataloader:\n",
    "  with torch.no_grad():\n",
    "    preds = model.generate(\n",
    "      batch[\"input_ids\"],\n",
    "      num_beams=15,\n",
    "      num_return_sequences=1,\n",
    "      no_repeat_ngram_size=1,\n",
    "      remove_invalid_values=True,\n",
    "      max_length=128,\n",
    "    )\n",
    "  labels = batch[\"labels\"]\n",
    "  inputs = batch[\"input_ids\"]\n",
    "  break\n",
    "\n",
    "# Replace -100 (see above)\n",
    "inputs = np.where(inputs != -100, inputs, tokenizer.pad_token_id)\n",
    "labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "# Convert id tokens to text\n",
    "text_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "text_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "text_inputs = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "#print(bleu_score(list(text_labels.split()),list(text_preds.split())))\n",
    "\n",
    "# Show result\n",
    "print(\"***** Input's Text *****\")\n",
    "print(text_inputs[2])\n",
    "print(\"***** summmary (True Value) *****\")\n",
    "print(text_labels[2])\n",
    "print(\"***** summary (Generated Text) *****\")\n",
    "print(text_preds[2])\n",
    "\n",
    "# %%\n",
    "for i in range(5):\n",
    "    print(\"***** Input's Text *****\")\n",
    "    print(text_inputs[i])\n",
    "    print(\"***** summarry (True Value) *****\")\n",
    "    print(text_labels[i])\n",
    "    print(\"***** summary (Generated Text) *****\")\n",
    "    print(text_preds[i])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
